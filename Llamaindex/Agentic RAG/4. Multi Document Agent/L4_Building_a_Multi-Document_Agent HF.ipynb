{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b523e0a",
   "metadata": {},
   "source": [
    "# Lesson 4: Building a Multi-Document Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a323703",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3221a474-5817-4db2-af46-e029042a75a5",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20adaa26",
   "metadata": {},
   "source": [
    "## 1. Setup an agent over 3 papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b71ff6",
   "metadata": {},
   "source": [
    "**Note**: The pdf files are included with this lesson. To access these papers, go to the `File` menu and select`Open...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed10a24b-d65c-4b98-a93a-94ccdb8900d0",
   "metadata": {
    "height": 200,
    "tags": []
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d8f3185-3221-4b00-bd38-41d36e4a3307",
   "metadata": {
    "height": 149,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: metagpt.pdf\n",
      "Getting tools for paper: longlora.pdf\n",
      "Getting tools for paper: selfrag.pdf\n"
     ]
    }
   ],
   "source": [
    "from utils import get_doc_tools\n",
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e541bdd-14e1-41b6-81b5-b1bfda078d07",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<llama_index.core.tools.function_tool.FunctionTool object at 0x000001EC47668760>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x000001EC47669BD0>, <llama_index.core.tools.function_tool.FunctionTool object at 0x000001EC47C00A00>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x000001EC47C02DD0>, <llama_index.core.tools.function_tool.FunctionTool object at 0x000001EC476CBCD0>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x000001EC476C8FA0>]\n"
     ]
    }
   ],
   "source": [
    "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]\n",
    "print(initial_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bff58c52",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ghost\\AppData\\Local\\Temp\\ipykernel_16388\\4153170195.py:10: DeprecationWarning: Call to deprecated class HuggingFaceInferenceAPI. (Deprecated in favor of `HuggingFaceInferenceAPI` from `llama-index-llms-huggingface-api` which should be used instead.)\n",
      "  llm = HuggingFaceInferenceAPI(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.huggingface import HuggingFaceInferenceAPI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../../.env\")\n",
    "\n",
    "token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "llm = HuggingFaceInferenceAPI(\n",
    "    model_name=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    token=token,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "451341e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.gemini import Gemini\n",
    "llm = Gemini()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f2c6a9f",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(initial_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a124a438-5609-402e-8642-69d1088cb9ad",
   "metadata": {
    "height": 166,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    initial_tools, \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17409d4c-05a9-4bf4-b74f-75135fa3cb6b",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in LongLoRA, and then tell me about the evaluation results\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation dataset\"}\n",
      "=== Function Output ===\n",
      "PG19\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation results\"}\n",
      "=== Function Output ===\n",
      "The evaluation results show that the model achieves reasonable passkey retrieval accuracy until around 33k to 34k context length. By extending the position interpolation to 48k, the model can handle longer documents, presenting moderate retrieval ability (60%-90% accuracy) in the range of 33k to 45k. However, there is a sharp accuracy degradation observed after the 4k context length for the Llama2 7B model, even with the position interpolation extended.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in LongLoRA is not explicitly mentioned in the paper. However, the evaluation results show that the model achieves reasonable passkey retrieval accuracy until around 33k to 34k context length. By extending the position interpolation to 48k, the model can handle longer documents, presenting moderate retrieval ability (60%-90% accuracy) in the range of 33k to 45k. However, there is a sharp accuracy degradation observed after the 4k context length for the Llama2 7B model, even with the position interpolation extended.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used in LongLoRA, \"\n",
    "    \"and then tell me about the evaluation results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ace340b1-761f-4058-be41-68cf131541e4",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Give me a summary of both Self-RAG and LongLoRA\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_selfrag with args: {\"input\": \"Self-RAG\"}\n",
      "=== Function Output ===\n",
      "Self-RAG is a framework that enhances the quality and factuality of a large language model through retrieval and self-reflection. It involves training a single arbitrary language model to adaptively retrieve passages on-demand, generate and reflect on retrieved passages and its own generations using special tokens called reflection tokens. This framework enables the language model to tailor its behavior to diverse task requirements during the inference phase, significantly outperforming state-of-the-art language models and retrieval-augmented models on various tasks by improving factuality and citation accuracy for long-form generations.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA\"}\n",
      "=== Function Output ===\n",
      "LongLoRA is an efficient method for extending the context lengths of Large Language Models (LLMs) while minimizing computational costs and training time. It combines shifted sparse attention (S2-Attn) with LoRA to enable fine-tuning of models to longer context lengths without altering the original architectures significantly. LongLoRA has shown strong empirical results across various tasks and is compatible with existing techniques like Flash-Attention2. It allows researchers to extend LLMs' context efficiently, making it a valuable tool for working with large language models.\n",
      "=== LLM Response ===\n",
      "Self-RAG is a framework that enhances the quality and factuality of a large language model through retrieval and self-reflection. It involves training a single arbitrary language model to adaptively retrieve passages on-demand, generate and reflect on retrieved passages and its own generations using special tokens called reflection tokens. This framework enables the language model to tailor its behavior to diverse task requirements during the inference phase, significantly outperforming state-of-the-art language models and retrieval-augmented models on various tasks by improving factuality and citation accuracy for long-form generations.\n",
      "\n",
      "LongLoRA is an efficient method for extending the context lengths of Large Language Models (LLMs) while minimizing computational costs and training time. It combines shifted sparse attention (S2-Attn) with LoRA to enable fine-tuning of models to longer context lengths without altering the original architectures significantly. LongLoRA has shown strong empirical results across various tasks and is compatible with existing techniques like Flash-Attention2. It allows researchers to extend LLMs' context efficiently, making it a valuable tool for working with large language models.\n",
      "assistant: Self-RAG is a framework that enhances the quality and factuality of a large language model through retrieval and self-reflection. It involves training a single arbitrary language model to adaptively retrieve passages on-demand, generate and reflect on retrieved passages and its own generations using special tokens called reflection tokens. This framework enables the language model to tailor its behavior to diverse task requirements during the inference phase, significantly outperforming state-of-the-art language models and retrieval-augmented models on various tasks by improving factuality and citation accuracy for long-form generations.\n",
      "\n",
      "LongLoRA is an efficient method for extending the context lengths of Large Language Models (LLMs) while minimizing computational costs and training time. It combines shifted sparse attention (S2-Attn) with LoRA to enable fine-tuning of models to longer context lengths without altering the original architectures significantly. LongLoRA has shown strong empirical results across various tasks and is compatible with existing techniques like Flash-Attention2. It allows researchers to extend LLMs' context efficiently, making it a valuable tool for working with large language models.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\"Give me a summary of both Self-RAG and LongLoRA\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eede70c",
   "metadata": {},
   "source": [
    "## 2. Setup an agent over 11 papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18771e69",
   "metadata": {},
   "source": [
    "### Download 11 ICLR papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60d01d2c-547f-4054-b0fe-ed9b1a9cc3b5",
   "metadata": {
    "height": 472,
    "tags": []
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=LzPWWPAdY4\",\n",
    "    \"https://openreview.net/pdf?id=VTF8yNQM66\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "    \"https://openreview.net/pdf?id=9WD9KwssyT\",\n",
    "    \"https://openreview.net/pdf?id=yV6fD7LYkF\",\n",
    "    \"https://openreview.net/pdf?id=hnrB5YHoYu\",\n",
    "    \"https://openreview.net/pdf?id=WbWtOYIzIK\",\n",
    "    \"https://openreview.net/pdf?id=c5pwL0Soay\",\n",
    "    \"https://openreview.net/pdf?id=TpD2aG1h0D\"\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"loftq.pdf\",\n",
    "    \"swebench.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "    \"zipformer.pdf\",\n",
    "    \"values.pdf\",\n",
    "    \"finetune_fair_diffusion.pdf\",\n",
    "    \"knowledge_card.pdf\",\n",
    "    \"metra.pdf\",\n",
    "    \"vr_mcl.pdf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77426cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "To download these papers, below is the needed code:\n",
    "\n",
    "\n",
    "    #for url, paper in zip(urls, papers):\n",
    "         #!wget \"{url}\" -O \"{paper}\"\n",
    "    \n",
    "    \n",
    "**Note**: The pdf files are included with this lesson. To access these papers, go to the `File` menu and select`Open...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea5ee34d-02ac-4537-ae20-7ef6c5767172",
   "metadata": {
    "height": 149,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: metagpt.pdf\n",
      "Getting tools for paper: longlora.pdf\n",
      "Getting tools for paper: loftq.pdf\n",
      "Getting tools for paper: swebench.pdf\n",
      "Getting tools for paper: selfrag.pdf\n",
      "Getting tools for paper: zipformer.pdf\n",
      "Getting tools for paper: values.pdf\n",
      "Getting tools for paper: finetune_fair_diffusion.pdf\n",
      "Getting tools for paper: knowledge_card.pdf\n",
      "Getting tools for paper: metra.pdf\n",
      "Getting tools for paper: vr_mcl.pdf\n"
     ]
    }
   ],
   "source": [
    "from utils import get_doc_tools\n",
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e35d52c",
   "metadata": {},
   "source": [
    "### Extend the Agent with Tool Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20154923-873e-4941-9a3a-4926ab5f9b8c",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "671582f9-70d7-4a8f-b813-58b2a068ca72",
   "metadata": {
    "height": 149,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define an \"object\" index and retriever over these tools\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    all_tools,\n",
    "    index_cls=VectorStoreIndex,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3929882-e9dc-46ca-b495-53e3ed60340e",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "obj_retriever = obj_index.as_retriever(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba9cfecd-fe14-4da8-b9ba-b3d485d98a03",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tools = obj_retriever.retrieve(\n",
    "    \"Tell me about the eval dataset used in MetaGPT and SWE-Bench\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c270ffbf-69c7-48ea-a028-9ba25221cde5",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMetadata(description='Useful for summarization questions related to swebench', name='summary_tool_swebench', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools[2].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9cc0a0b6-9858-4348-9ae0-1cd4160f3fb7",
   "metadata": {
    "height": 268,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tool_retriever=obj_retriever,\n",
    "    llm=llm, \n",
    "    system_prompt=\"\"\" \\\n",
    "You are an agent designed to answer queries over a set of given papers.\n",
    "Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
    "\n",
    "\"\"\",\n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a250cf1a-e011-4994-bcca-4e0294f20864",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in MetaGPT and compare it against SWE-Bench\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metagpt with args: {\"input\": \"evaluation dataset used in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in MetaGPT includes HumanEval, MBPP, and SoftwareDev.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_swebench with args: {\"input\": \"evaluation dataset used in SWE-Bench\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in SWE-Bench consists of task instances collected from real GitHub issues and pull requests across popular Python repositories. It includes task instructions, issue text, retrieved files and documentation, example patch files, and prompts for generating patch files. Models like ChatGPT-3.5, GPT-4, Claude 2, and SWE-Llama were evaluated based on their ability to resolve issues, with Claude 2 being the best performer. The dataset is designed to be challenging for evaluating language models in software engineering tasks and aims to cover more programming languages and domains in the future. Task instances are validated through execution-based verification to ensure usability and non-triviality, and the dataset is constructed by scraping pull requests from top PyPI libraries. Task instances are finalized after passing execution-based validation steps and are used to evaluate models by comparing test-to-status mappings with ground truth test results. The dataset includes tasks from various GitHub repositories, covering a range of software engineering challenges like handling specific file types, resolving errors, and correcting coding issues.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in MetaGPT includes HumanEval, MBPP, and SoftwareDev. \n",
      "\n",
      "On the other hand, the evaluation dataset used in SWE-Bench consists of task instances collected from real GitHub issues and pull requests across popular Python repositories. It includes task instructions, issue text, retrieved files and documentation, example patch files, and prompts for generating patch files. Models like ChatGPT-3.5, GPT-4, Claude 2, and SWE-Llama were evaluated based on their ability to resolve issues, with Claude 2 being the best performer. The dataset is designed to be challenging for evaluating language models in software engineering tasks and aims to cover more programming languages and domains in the future. Task instances are validated through execution-based verification to ensure usability and non-triviality, and the dataset is constructed by scraping pull requests from top PyPI libraries. Task instances are finalized after passing execution-based validation steps and are used to evaluate models by comparing test-to-status mappings with ground truth test results. The dataset includes tasks from various GitHub repositories, covering a range of software engineering challenges like handling specific file types, resolving errors, and correcting coding issues.\n",
      "assistant: The evaluation dataset used in MetaGPT includes HumanEval, MBPP, and SoftwareDev. \n",
      "\n",
      "On the other hand, the evaluation dataset used in SWE-Bench consists of task instances collected from real GitHub issues and pull requests across popular Python repositories. It includes task instructions, issue text, retrieved files and documentation, example patch files, and prompts for generating patch files. Models like ChatGPT-3.5, GPT-4, Claude 2, and SWE-Llama were evaluated based on their ability to resolve issues, with Claude 2 being the best performer. The dataset is designed to be challenging for evaluating language models in software engineering tasks and aims to cover more programming languages and domains in the future. Task instances are validated through execution-based verification to ensure usability and non-triviality, and the dataset is constructed by scraping pull requests from top PyPI libraries. Task instances are finalized after passing execution-based validation steps and are used to evaluate models by comparing test-to-status mappings with ground truth test results. The dataset includes tasks from various GitHub repositories, covering a range of software engineering challenges like handling specific file types, resolving errors, and correcting coding issues.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used \"\n",
    "    \"in MetaGPT and compare it against SWE-Bench\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8084c8cb-98ed-4835-aaa4-5b0c7254be6d",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Compare and contrast the LoRA papers (LongLoRA, LoftQ). Analyze the approach in each paper first. \n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"Compare and contrast the approach in the LongLoRA paper.\"}\n",
      "=== Function Output ===\n",
      "The approach in the LongLoRA paper introduces an efficient fine-tuning method that focuses on extending the context sizes of pre-trained large language models (LLMs) while minimizing computational costs. It utilizes Shifted Sparse Attention (S2-Attn) during training to approximate long contexts efficiently, while retaining the original attention architecture during inference. This method combines S2-Attn with Low-rank Adaptation (LoRA) to achieve strong empirical results on various tasks and context lengths, showcasing improved training speed and memory efficiency while maintaining performance. In contrast, traditional Low-rank Adaptation (LoRA) alone is found to be less effective and efficient for context extension compared to LongLoRA. Additionally, LongLoRA introduces an improved version of LoRA, denoted as LoRA+, which opens up embedding and normalization layers for training, proving crucial for long-context adaptation and bridging the performance gap with full fine-tuning. The LongLoRA approach stands out for its innovative attention mechanisms, fine-tuning strategies, and balance of efficiency, accuracy, and memory optimization in handling long-context tasks.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_loftq with args: {\"input\": \"Compare and contrast the approach in the LoftQ paper.\"}\n",
      "=== Function Output ===\n",
      "The approach in the LoftQ paper focuses on combining quantization and low-rank approximation techniques for pre-trained models, aiming to bridge the performance gap between full fine-tuning and quantization plus LoRA fine-tuning approaches. LoftQ integrates low-rank approximation with quantization in an alternating optimization process to provide a beneficial starting point for subsequent LoRA fine-tuning. This approach helps improve generalization in downstream tasks by effectively preserving the starting point of pre-trained weights and achieving robust convergence.\n",
      "\n",
      "In contrast, the traditional approach exemplified by QLoRA primarily emphasizes quantization techniques and may overlook the importance of subsequent LoRA fine-tuning. QLoRA attaches zero-initialized low-rank adapters to the quantized pre-trained model, potentially leading to performance degradation, especially in low-bit scenarios. LoftQ, on the other hand, actively addresses the quantization discrepancy by integrating quantization and low-rank approximation, consistently outperforming QLoRA across all precision levels and excelling in low-bit scenarios. LoftQ's strategy of combining quantization and low-rank approximation sets it apart from traditional quantization methods, showcasing its potential for enhancing downstream task adaptation with large language models.\n",
      "=== LLM Response ===\n",
      "In the LongLoRA paper, the approach introduces an efficient fine-tuning method that extends the context sizes of pre-trained large language models (LLMs) while minimizing computational costs. It utilizes Shifted Sparse Attention (S2-Attn) during training to approximate long contexts efficiently and combines it with Low-rank Adaptation (LoRA) to achieve strong empirical results on various tasks and context lengths. LongLoRA introduces an improved version of LoRA, denoted as LoRA+, which opens up embedding and normalization layers for training, crucial for long-context adaptation and performance improvement.\n",
      "\n",
      "On the other hand, the LoftQ paper focuses on combining quantization and low-rank approximation techniques for pre-trained models to bridge the performance gap between full fine-tuning and quantization plus LoRA fine-tuning approaches. LoftQ integrates low-rank approximation with quantization in an alternating optimization process to provide a beneficial starting point for subsequent LoRA fine-tuning. This approach helps improve generalization in downstream tasks by effectively preserving the starting point of pre-trained weights and achieving robust convergence.\n",
      "\n",
      "In summary, LongLoRA emphasizes innovative attention mechanisms, fine-tuning strategies, and efficiency in handling long-context tasks, while LoftQ focuses on combining quantization and low-rank approximation techniques to enhance downstream task adaptation with large language models.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Compare and contrast the LoRA papers (LongLoRA, LoftQ). \"\n",
    "    \"Analyze the approach in each paper first. \"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
