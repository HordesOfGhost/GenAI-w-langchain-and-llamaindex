{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../../../.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The syntax of the command is incorrect.\n",
      "curl: (3) URL using bad/illegal format or missing URL\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p 'data/'\n",
    "!curl 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -o 'data/paul_graham_essay.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Llama-index nodes/chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents: 1\n",
      "Number of nodes: 57 with the current chunk size of 512\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data/\").load_data()\n",
    "node_parser = SimpleNodeParser.from_defaults(chunk_size=512)\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "\n",
    "# By default, the node/chunks ids are set to random uuids. To ensure same id's per run, we manually set them.\n",
    "for idx, node in enumerate(nodes):\n",
    "    node.id_ = f\"node_{idx}\"\n",
    "\n",
    "print(f\"Number of Documents: {len(documents)}\")\n",
    "print(f\"Number of nodes: {len(nodes)} with the current chunk size of {node_parser.chunk_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create local Deep Lake Vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ghost\\anaconda3\\envs\\genai360\\lib\\site-packages\\deeplake\\util\\check_latest_version.py:32: UserWarning: A newer version of deeplake (3.8.21) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, ServiceContext, StorageContext\n",
    "from llama_index.vector_stores.deeplake import DeepLakeVectorStore\n",
    "from llama_index.llms.huggingface import HuggingFaceInferenceAPI\n",
    "from llama_index.core import set_global_tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "llm = HuggingFaceInferenceAPI(\n",
    "    model_name='mistralai/Mistral-7B-Instruct-v0.2',\n",
    "    generate_kwargs={'temperature':0.5,\"max_length\": 64,\"max_new_tokens\":512}\n",
    ")\n",
    "\n",
    "set_global_tokenizer(\n",
    "    AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\").encode\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ghost\\AppData\\Local\\Temp\\ipykernel_6236\\4240787486.py:5: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(embed_model=\"local\", llm=llm,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "806c943fc53d4ed7990df2ec994ff5dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading data to deeplake dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:00<00:00, 383.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='data/deep_lake_db', tensors=['text', 'metadata', 'embedding', 'id'])\n",
      "\n",
      "  tensor      htype      shape     dtype  compression\n",
      "  -------    -------    -------   -------  ------- \n",
      "   text       text      (57, 1)     str     None   \n",
      " metadata     json      (57, 1)     str     None   \n",
      " embedding  embedding  (57, 384)  float32   None   \n",
      "    id        text      (57, 1)     str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "\n",
    "# Create a DeepLakeVectorStore locally to store the vectors\n",
    "dataset_path = \"data/deep_lake_db\"\n",
    "vector_store = DeepLakeVectorStore(dataset_path=dataset_path, overwrite=True)\n",
    "\n",
    "service_context = ServiceContext.from_defaults(embed_model=\"local\", llm=llm,)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "vector_index = VectorStoreIndex(nodes, service_context=service_context, storage_context=storage_context, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload local Vector Database to ActiveLoop's Platform and convert to managed database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying dataset: 96%|█████████▋| 27/28 [00:19<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/thapabibek1129/optimization_paul_graham\n",
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying dataset: 96%|█████████▋| 27/28 [01:06<00:02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/thapabibek1129/optimization_paul_graham_managed\n",
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset(path='hub://thapabibek1129/optimization_paul_graham_managed', tensors=['embedding', 'id', 'metadata', 'text'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import deeplake\n",
    "local = \"./data/deep_lake_db\"\n",
    "hub_path = \"hub://thapabibek1129/optimization_paul_graham\"\n",
    "hub_managed_path = \"hub://thapabibek1129/optimization_paul_graham_managed\"\n",
    "\n",
    "# First upload our local vector store\n",
    "deeplake.deepcopy(local, hub_path, overwrite=True)\n",
    "# Create a managed vector store under a different name\n",
    "deeplake.deepcopy(hub_path, hub_managed_path, overwrite=True, runtime={\"tensor_db\": True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate a Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://thapabibek1129/optimization_paul_graham_managed already exists, loading from the storage\n"
     ]
    }
   ],
   "source": [
    "db = DeepLakeVectorStore(dataset_path=hub_managed_path, overwrite=False, read_only=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DeepLakeVectorStore' object has no attribute 'vectorstore'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Fetch dataset docs and ids \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[43mdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorstore\u001b[49m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mdata(fetch_chunks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, aslist\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      3\u001b[0m ids \u001b[38;5;241m=\u001b[39m db\u001b[38;5;241m.\u001b[39mvectorstore\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mid\u001b[38;5;241m.\u001b[39mdata(fetch_chunks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, aslist\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(docs))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DeepLakeVectorStore' object has no attribute 'vectorstore'"
     ]
    }
   ],
   "source": [
    "# Fetch dataset docs and ids \n",
    "docs = db.vectorstore.dataset.text.data(fetch_chunks=True, aslist=True)['value']\n",
    "ids = db.vectorstore.dataset.id.data(fetch_chunks=True, aslist=True)['value']\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Dataset docs and ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/thapabibek1129/optimization_paul_graham_managed\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hub://thapabibek1129/optimization_paul_graham_managed loaded successfully.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " "
     ]
    }
   ],
   "source": [
    "ds = deeplake.load('hub://thapabibek1129/optimization_paul_graham_managed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://thapabibek1129/optimization_paul_graham_managed already exists, loading from the storage\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import DeepLake\n",
    "vector_store = DeepLake(dataset_path='hub://thapabibek1129/optimization_paul_graham_managed', read_only=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'VectorStore' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[43mvector_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[38;5;241m.\u001b[39mdata()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      2\u001b[0m ids \u001b[38;5;241m=\u001b[39m vector_store\u001b[38;5;241m.\u001b[39mvectorstore\u001b[38;5;241m.\u001b[39mid\u001b[38;5;241m.\u001b[39mdata()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'VectorStore' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "docs = vector_store.vectorstore.text.data()['value']\n",
    "ids = vector_store.vectorstore.id.data()['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_handler': <deeplake.core.vectorstore.dataset_handlers.client_side_dataset_handler.ClientSideDH at 0x27b0b7ca9d0>,\n",
       " 'deep_memory': <deeplake.core.vectorstore.deep_memory.deep_memory.DeepMemory at 0x27b0b2a17c0>}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.vectorstore.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Synthetic training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import H\n",
    "client = HfApi()\n",
    "def generate_question(text):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model_name='mistralai/Mistral-7B-Instruct-v0.2',\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a world class expert for generating questions based on provided context. \\\n",
    "                        You make sure the question can be answered by the text.\"},\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": text,\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        question_string = \"No question generated\"\n",
    "        return question_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'HfApi' object has no attribute 'chat'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'No question generated'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_question(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepLakeVectorStore(stores_text=True, is_embedding_query=True, flat_metadata=True, ingestion_batch_size=1024, num_workers=4, token=None, read_only=True, dataset_path='hub://thapabibek1129/optimization_paul_graham_managed')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_queries(docs: list[str], ids: list[str], n: int):\n",
    "\n",
    "    questions = []\n",
    "    relevances = []\n",
    "    pbar = tqdm(total=n)\n",
    "    while len(questions) < n:\n",
    "        # 1. randomly draw a piece of text and relevance id\n",
    "        r = random.randint(0, len(docs)-1)\n",
    "        text, label = docs[r], ids[r]\n",
    "\n",
    "        # 2. generate queries and assign and relevance id\n",
    "        generated_qs = [generate_question(text)]\n",
    "        if generated_qs == [\"No question generated\"]:\n",
    "            print(\"No question generated\")\n",
    "            continue\n",
    "\n",
    "        questions.extend(generated_qs)\n",
    "        relevances.extend([[(label, 1)] for _ in generated_qs])\n",
    "        pbar.update(len(generated_qs))\n",
    "\n",
    "    return questions[:n], relevances[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch Query Generation Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions, relevances = generate_queries(docs, ids, n=40)\n",
    "print(len(questions)) #40\n",
    "print(questions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch Deep Memory Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "openai_embeddings = OpenAIEmbeddings()\n",
    "\n",
    "job_id = db.vectorstore.deep_memory.train(\n",
    "    queries=questions,\n",
    "    relevance=relevances,\n",
    "    embedding_function=openai_embeddings.embed_documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Deep Memory Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# During training you can check the status of the training run\n",
    "db.vectorstore.deep_memory.status(job_id=\"657b3083d528b0fd224173c6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Deep Memory-enabled inference by setting deep_memory=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import OpenAI\n",
    "query = \"What are the main things Paul worked on before college?\"\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-1106\")\n",
    "embed_model = OpenAIEmbedding()\n",
    "\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=llm,)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "db = DeepLakeVectorStore(dataset_path=hub_managed_path, overwrite=False, read_only=True,)\n",
    "vector_index = VectorStoreIndex.from_vector_store(db, service_context=service_context, storage_context=storage_context, show_progress=True)\n",
    "\n",
    "query_engine = vector_index.as_query_engine(similarity_top_k=3, vector_store_kwargs={\"deep_memory\": True})\n",
    "response_vector = query_engine.query(query)\n",
    "print(response_vector.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, let's run a quantitative evaluation on another set of synthetically generated test queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate validation queries\n",
    "validation_questions, validation_relevances = generate_queries(docs, ids, n=40)\n",
    "\n",
    "# Launch the evaluation function\n",
    "recalls = db.vectorstore.deep_memory.evaluate(\n",
    "    queries=validation_questions,\n",
    "    relevance=validation_relevances,\n",
    "    embedding_function=openai_embeddings.embed_documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai360",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
