{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../../../.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BaseLine Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents: 1\n",
      "Number of nodes: 57 with the current chunk size of 512\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# First we create Document LlamaIndex objects from the text data\n",
    "documents = SimpleDirectoryReader(\"./data/\").load_data()\n",
    "node_parser = SimpleNodeParser.from_defaults(chunk_size=512)\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "\n",
    "# By default, the node/chunks ids are set to random uuids. To ensure same id's per run, we manually set them.\n",
    "for idx, node in enumerate(nodes):\n",
    "    node.id_ = f\"node_{idx}\"\n",
    "\n",
    "print(f\"Number of Documents: {len(documents)}\")\n",
    "print(f\"Number of nodes: {len(nodes)} with the current chunk size of {node_parser.chunk_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "from llama_index.core import set_global_tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id='mistralai/Mistral-7B-Instruct-v0.2',\n",
    "    model_kwargs={'temperature':0.5,\"max_length\": 64,\"max_new_tokens\":512}\n",
    ")\n",
    "\n",
    "set_global_tokenizer(\n",
    "    AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\").encode\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ghost\\AppData\\Local\\Temp\\ipykernel_5680\\2720333910.py:14: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(embed_model='local', llm=llm,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b46602948464da7bb591e2ccbf0603a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading data to deeplake dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:00<00:00, 292.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='./data/paul_graham/deep_lake_db', tensors=['text', 'metadata', 'embedding', 'id'])\n",
      "\n",
      "  tensor      htype      shape     dtype  compression\n",
      "  -------    -------    -------   -------  ------- \n",
      "   text       text      (57, 1)     str     None   \n",
      " metadata     json      (57, 1)     str     None   \n",
      " embedding  embedding  (57, 384)  float32   None   \n",
      "    id        text      (57, 1)     str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, ServiceContext, StorageContext\n",
    "from llama_index.vector_stores.deeplake import DeepLakeVectorStore\n",
    "\n",
    "\n",
    "# Create a local Deep Lake VectorStore\n",
    "dataset_path = \"./data/paul_graham/deep_lake_db\"\n",
    "vector_store = DeepLakeVectorStore(dataset_path=dataset_path, overwrite=True)\n",
    "\n",
    "# # LLM that will answer questions with the retrieved context\n",
    "# llm = OpenAI(model=\"gpt-3.5-turbo-1106\")\n",
    "# # We use OpenAI's embedding model \"text-embedding-ada-002\"\n",
    "# embed_model = OpenAIEmbedding()\n",
    "\n",
    "service_context = ServiceContext.from_defaults(embed_model='local', llm=llm,)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "vector_index = VectorStoreIndex(nodes, service_context=service_context, storage_context=storage_context, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Calculated available context size -309 was not non-negative.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m query_engine \u001b[38;5;241m=\u001b[39m vector_index\u001b[38;5;241m.\u001b[39mas_query_engine(similarity_top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m response_vector \u001b[38;5;241m=\u001b[39m \u001b[43mquery_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat are the main things Paul worked on before college?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(response_vector\u001b[38;5;241m.\u001b[39mresponse)\n",
      "File \u001b[1;32mc:\\Users\\Ghost\\anaconda3\\envs\\genai360\\lib\\site-packages\\llama_index\\core\\base\\base_query_engine.py:40\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[1;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     39\u001b[0m     str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_or_query_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ghost\\anaconda3\\envs\\genai360\\lib\\site-packages\\llama_index\\core\\query_engine\\retriever_query_engine.py:187\u001b[0m, in \u001b[0;36mRetrieverQueryEngine._query\u001b[1;34m(self, query_bundle)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    184\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mQUERY, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_bundle\u001b[38;5;241m.\u001b[39mquery_str}\n\u001b[0;32m    185\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m query_event:\n\u001b[0;32m    186\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve(query_bundle)\n\u001b[1;32m--> 187\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_response_synthesizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynthesize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_bundle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    192\u001b[0m     query_event\u001b[38;5;241m.\u001b[39mon_end(payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mRESPONSE: response})\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\Ghost\\anaconda3\\envs\\genai360\\lib\\site-packages\\llama_index\\core\\response_synthesizers\\base.py:180\u001b[0m, in \u001b[0;36mBaseSynthesizer.synthesize\u001b[1;34m(self, query, nodes, additional_source_nodes, **response_kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m     query \u001b[38;5;241m=\u001b[39m QueryBundle(query_str\u001b[38;5;241m=\u001b[39mquery)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    178\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mSYNTHESIZE, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query\u001b[38;5;241m.\u001b[39mquery_str}\n\u001b[0;32m    179\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[1;32m--> 180\u001b[0m     response_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_response(\n\u001b[0;32m    181\u001b[0m         query_str\u001b[38;5;241m=\u001b[39mquery\u001b[38;5;241m.\u001b[39mquery_str,\n\u001b[0;32m    182\u001b[0m         text_chunks\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m    183\u001b[0m             n\u001b[38;5;241m.\u001b[39mnode\u001b[38;5;241m.\u001b[39mget_content(metadata_mode\u001b[38;5;241m=\u001b[39mMetadataMode\u001b[38;5;241m.\u001b[39mLLM) \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nodes\n\u001b[0;32m    184\u001b[0m         ],\n\u001b[0;32m    185\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs,\n\u001b[0;32m    186\u001b[0m     )\n\u001b[0;32m    188\u001b[0m     additional_source_nodes \u001b[38;5;241m=\u001b[39m additional_source_nodes \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[0;32m    189\u001b[0m     source_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(nodes) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(additional_source_nodes)\n",
      "File \u001b[1;32mc:\\Users\\Ghost\\anaconda3\\envs\\genai360\\lib\\site-packages\\llama_index\\core\\response_synthesizers\\compact_and_refine.py:38\u001b[0m, in \u001b[0;36mCompactAndRefine.get_response\u001b[1;34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# use prompt helper to fix compact text_chunks under the prompt limitation\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# TODO: This is a temporary fix - reason it's temporary is that\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# the refine template does not account for size of previous answer.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m new_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_compact_text_chunks(query_str, text_chunks)\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mget_response(\n\u001b[0;32m     39\u001b[0m     query_str\u001b[38;5;241m=\u001b[39mquery_str,\n\u001b[0;32m     40\u001b[0m     text_chunks\u001b[38;5;241m=\u001b[39mnew_texts,\n\u001b[0;32m     41\u001b[0m     prev_response\u001b[38;5;241m=\u001b[39mprev_response,\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs,\n\u001b[0;32m     43\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Ghost\\anaconda3\\envs\\genai360\\lib\\site-packages\\llama_index\\core\\response_synthesizers\\refine.py:165\u001b[0m, in \u001b[0;36mRefine.get_response\u001b[1;34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[0m\n\u001b[0;32m    160\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_give_response_single(\n\u001b[0;32m    161\u001b[0m             query_str, text_chunk, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs\n\u001b[0;32m    162\u001b[0m         )\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;66;03m# refine response if possible\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_refine_response_single(\n\u001b[0;32m    166\u001b[0m             prev_response, query_str, text_chunk, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs\n\u001b[0;32m    167\u001b[0m         )\n\u001b[0;32m    168\u001b[0m     prev_response \u001b[38;5;241m=\u001b[39m response\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Ghost\\anaconda3\\envs\\genai360\\lib\\site-packages\\llama_index\\core\\response_synthesizers\\refine.py:273\u001b[0m, in \u001b[0;36mRefine._refine_response_single\u001b[1;34m(self, response, query_str, text_chunk, **response_kwargs)\u001b[0m\n\u001b[0;32m    266\u001b[0m refine_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_refine_template\u001b[38;5;241m.\u001b[39mpartial_format(\n\u001b[0;32m    267\u001b[0m     query_str\u001b[38;5;241m=\u001b[39mquery_str, existing_answer\u001b[38;5;241m=\u001b[39mresponse\n\u001b[0;32m    268\u001b[0m )\n\u001b[0;32m    270\u001b[0m \u001b[38;5;66;03m# compute available chunk size to see if there is any available space\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;66;03m# determine if the refine template is too big (which can happen if\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;66;03m# prompt template + query + existing answer is too large)\u001b[39;00m\n\u001b[1;32m--> 273\u001b[0m avail_chunk_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prompt_helper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_available_chunk_size\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrefine_template\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m avail_chunk_size \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;66;03m# if the available chunk size is negative, then the refine template\u001b[39;00m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;66;03m# is too big and we just return the original response\u001b[39;00m\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\Ghost\\anaconda3\\envs\\genai360\\lib\\site-packages\\llama_index\\core\\indices\\prompt_helper.py:217\u001b[0m, in \u001b[0;36mPromptHelper._get_available_chunk_size\u001b[1;34m(self, prompt, num_chunks, padding, llm)\u001b[0m\n\u001b[0;32m    214\u001b[0m     prompt_str \u001b[38;5;241m=\u001b[39m get_empty_prompt_txt(prompt)\n\u001b[0;32m    215\u001b[0m     num_prompt_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token_counter\u001b[38;5;241m.\u001b[39mget_string_tokens(prompt_str)\n\u001b[1;32m--> 217\u001b[0m available_context_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_available_context_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_prompt_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m result \u001b[38;5;241m=\u001b[39m available_context_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_chunks \u001b[38;5;241m-\u001b[39m padding\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size_limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Ghost\\anaconda3\\envs\\genai360\\lib\\site-packages\\llama_index\\core\\indices\\prompt_helper.py:149\u001b[0m, in \u001b[0;36mPromptHelper._get_available_context_size\u001b[1;34m(self, num_prompt_tokens)\u001b[0m\n\u001b[0;32m    147\u001b[0m context_size_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_window \u001b[38;5;241m-\u001b[39m num_prompt_tokens \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_output\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context_size_tokens \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    150\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculated available context size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext_size_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m was\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    151\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not non-negative.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    152\u001b[0m     )\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m context_size_tokens\n",
      "\u001b[1;31mValueError\u001b[0m: Calculated available context size -309 was not non-negative."
     ]
    }
   ],
   "source": [
    "query_engine = vector_index.as_query_engine(similarity_top_k=10)\n",
    "response_vector = query_engine.query(\"What are the main things Paul worked on before college?\")\n",
    "print(response_vector.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llama_index.evaluation'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate_question_context_pairs\n\u001b[0;32m      2\u001b[0m qc_dataset \u001b[38;5;241m=\u001b[39m generate_question_context_pairs(\n\u001b[0;32m      3\u001b[0m     nodes,\n\u001b[0;32m      4\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[0;32m      5\u001b[0m     num_questions_per_chunk\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# We can save the dataset as a json file for later use.\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'llama_index.evaluation'"
     ]
    }
   ],
   "source": [
    "from llama_index.evaluation import generate_question_context_pairs\n",
    "qc_dataset = generate_question_context_pairs(\n",
    "    nodes,\n",
    "    llm=llm,\n",
    "    num_questions_per_chunk=1\n",
    ")\n",
    "# We can save the dataset as a json file for later use.\n",
    "qc_dataset.save_json(\"qc_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.finetuning.embeddings.common import (\n",
    "    EmbeddingQAFinetuneDataset,\n",
    ")\n",
    "qc_dataset = EmbeddingQAFinetuneDataset.from_json(\n",
    "    \"qc_dataset.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def display_results_retriever(name, eval_results):\n",
    "    \"\"\"Display results from evaluate.\"\"\"\n",
    "\n",
    "    metric_dicts = []\n",
    "    for eval_result in eval_results:\n",
    "        metric_dict = eval_result.metric_vals_dict\n",
    "        metric_dicts.append(metric_dict)\n",
    "\n",
    "    full_df = pd.DataFrame(metric_dicts)\n",
    "\n",
    "    hit_rate = full_df[\"hit_rate\"].mean()\n",
    "    mrr = full_df[\"mrr\"].mean()\n",
    "\n",
    "    metric_df = pd.DataFrame(\n",
    "        {\"Retriever Name\": [name], \"Hit Rate\": [hit_rate], \"MRR\": [mrr]}\n",
    "    )\n",
    "\n",
    "    return metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.evaluation import RetrieverEvaluator\n",
    "\n",
    "# We can evaluate the retievers with different top_k values.\n",
    "for i in [2, 4, 6, 8, 10]:\n",
    "    retriever = vector_index.as_retriever(similarity_top_k=i)\n",
    "    retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "        [\"mrr\", \"hit_rate\"], retriever=retriever\n",
    "    )\n",
    "    eval_results = await retriever_evaluator.aevaluate_dataset(qc_dataset)\n",
    "    print(display_results_retriever(f\"Retriever top_{i}\", eval_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.evaluation import RelevancyEvaluator, FaithfulnessEvaluator, BatchEvalRunner\n",
    "\n",
    "for i in [2, 4, 6, 8, 10]:   \n",
    "    # Set Faithfulness and Relevancy evaluators\n",
    "    query_engine = vector_index.as_query_engine(similarity_top_k=i)\n",
    "\n",
    "    # While we use GPT3.5-Turbo to answer questions\n",
    "    # we can use GPT4 to evaluate the answers.\n",
    "    llm_gpt4 = OpenAI(temperature=0, model=\"gpt-4-1106-preview\")\n",
    "    service_context_gpt4 = ServiceContext.from_defaults(llm=llm_gpt4)\n",
    "\n",
    "    faithfulness_evaluator = FaithfulnessEvaluator(service_context=service_context_gpt4)\n",
    "    relevancy_evaluator = RelevancyEvaluator(service_context=service_context_gpt4)\n",
    "\n",
    "    # Run evaluation\n",
    "    queries = list(qc_dataset.queries.values())\n",
    "    batch_eval_queries = queries[:20]\n",
    "\n",
    "    runner = BatchEvalRunner(\n",
    "    {\"faithfulness\": faithfulness_evaluator, \"relevancy\": relevancy_evaluator},\n",
    "    workers=8,\n",
    "    )\n",
    "    eval_results = await runner.aevaluate_queries(\n",
    "        query_engine, queries=batch_eval_queries\n",
    "    )\n",
    "    faithfulness_score = sum(result.passing for result in eval_results['faithfulness']) / len(eval_results['faithfulness'])\n",
    "    print(f\"top_{i} faithfulness_score: {faithfulness_score}\")\n",
    "\n",
    "    relevancy_score = sum(result.passing for result in eval_results['faithfulness']) / len(eval_results['relevancy'])\n",
    "    print(f\"top_{i} relevancy_score: {relevancy_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index import VectorStoreIndex, ServiceContext, StorageContext\n",
    "from llama_index.vector_stores import DeepLakeVectorStore\n",
    "from llama_index.embeddings.cohereai import CohereEmbedding\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "# Create another local DeepLakeVectorStore to store the embeddings\n",
    "dataset_path = \"./data/paul_graham/deep_lake_db_1\"\n",
    "vector_store = DeepLakeVectorStore(dataset_path=dataset_path, overwrite=False)\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-1106\")\n",
    "embed_model = CohereEmbedding(\n",
    "    cohere_api_key=os.getenv('COHERE_API_KEY'),\n",
    "    model_name=\"embed-english-v3.0\",\n",
    "    input_type=\"search_document\",\n",
    ")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=llm,)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "vector_index = VectorStoreIndex(nodes, service_context=service_context, storage_context=storage_context, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.evaluation import RetrieverEvaluator\n",
    "\n",
    "embed_model.input_type = \"search_query\"\n",
    "retriever = vector_index.as_retriever(similarity_top_k=10, embed_model=embed_model)\n",
    "\n",
    "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "    [\"mrr\", \"hit_rate\"], retriever=retriever\n",
    ")\n",
    "eval_results = await retriever_evaluator.aevaluate_dataset(qc_dataset)\n",
    "print(display_results_retriever(f\"Retriever_cohere_embeds\", eval_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incorporating a Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "from llama_index.indices.postprocessor import SentenceTransformerRerank, LLMRerank\n",
    "\n",
    "st_reranker = SentenceTransformerRerank(\n",
    "    top_n=5, model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    ")\n",
    "\n",
    "llm_reranker = LLMRerank(\n",
    "    choice_batch_size=4, top_n=5,\n",
    ")\n",
    "cohere_rerank = CohereRerank(api_key=os.getenv('COHERE_API_KEY'), top_n=10)\n",
    "for reranker in [cohere_rerank, st_reranker, llm_reranker]:\n",
    "    retriever_with_reranker = vector_index.as_retriever(similarity_top_k=10, postprocessor=reranker, embed_model=embed_model)\n",
    "\n",
    "    retriever_evaluator_1 = RetrieverEvaluator.from_metric_names(\n",
    "        [\"mrr\", \"hit_rate\"], retriever=retriever_with_reranker\n",
    "    )\n",
    "    eval_results1 = await retriever_evaluator_1.aevaluate_dataset(qc_dataset)\n",
    "    print(display_results_retriever(\"Retriever with added Reranker\", eval_results1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Employing Deep Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_query_relevance(qa_dataset):\n",
    "    \"\"\"Function for converting LlamaIndex dataset to correct format for deep memory training\"\"\"\n",
    "    queries = [text for _, text in qa_dataset.queries.items()]\n",
    "    relevant_docs = qa_dataset.relevant_docs\n",
    "    relevance = []\n",
    "    for doc in relevant_docs:\n",
    "        relevance.append([(relevant_docs[doc][0], 1)])\n",
    "    return queries, relevance\n",
    "\n",
    "train_queries, train_relevance = create_query_relevance(qc_dataset)\n",
    "print(len(train_queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplake\n",
    "local = \"./data/paul_graham/deep_lake_db\"\n",
    "hub_path = \"hub://genai360/optimization_paul_graham\"\n",
    "hub_managed_path = \"hub://genai360/optimization_paul_graham_managed\"\n",
    "\n",
    "# First upload our local vector store\n",
    "deeplake.deepcopy(local, hub_path, overwrite=True)\n",
    "# Create a managed vector store\n",
    "deeplake.deepcopy(hub_path, hub_managed_path, overwrite=True, runtime={\"tensor_db\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index import VectorStoreIndex, ServiceContext, StorageContext\n",
    "from llama_index.vector_stores import DeepLakeVectorStore\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "vector_store = DeepLakeVectorStore(dataset_path=hub_managed_path, overwrite=False, runtime={\"tensor_db\": True}, read_only=True)\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-1106\")\n",
    "embed_model = OpenAIEmbedding()\n",
    "\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=llm,)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "vector_index = VectorStoreIndex.from_vector_store(vector_store,service_context=service_context, storage_context=storage_context, use_async=False, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "job_id = vector_store.vectorstore.deep_memory.train(\n",
    "    queries=train_queries,\n",
    "    relevance=train_relevance,\n",
    "    embedding_function=embeddings.embed_documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.vectorstore.deep_memory.status('652dceeed7d1579bf6abf3df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.evaluation import generate_question_context_pairs\n",
    "# Generate test dataset\n",
    "test_dataset = generate_question_context_pairs(\n",
    "    nodes[:20],\n",
    "    llm=llm,\n",
    "    num_questions_per_chunk=1\n",
    ")\n",
    "test_dataset.save_json(\"test_dataset.json\")\n",
    "\n",
    "# We can also load the dataset from a json file if already done previously.\n",
    "from llama_index.finetuning.embeddings.common import (\n",
    "    EmbeddingQAFinetuneDataset,\n",
    ")\n",
    "test_dataset = EmbeddingQAFinetuneDataset.from_json(\n",
    "    \"test_dataset.json\"\n",
    ")\n",
    "\n",
    "test_queries, test_relevance = create_query_relevance(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate recall on the generated test dataset\n",
    "recalls = vector_store.vectorstore.deep_memory.evaluate(\n",
    "    queries=test_queries,\n",
    "    relevance=test_relevance,\n",
    "    embedding_function=embeddings.embed_documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "from llama_index.evaluation import (\n",
    "    RetrieverEvaluator,\n",
    ")\n",
    "\n",
    "base_retriever = vector_index.as_retriever(similarity_top_k=10)\n",
    "deep_memory_retriever = vector_index.as_retriever(\n",
    "similarity_top_k=10, vector_store_kwargs={\"deep_memory\": True}\n",
    ")\n",
    "\n",
    "base_retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "    [\"mrr\", \"hit_rate\"], retriever=base_retriever\n",
    ")\n",
    "eval_results = await base_retriever_evaluator.aevaluate_dataset(test_dataset)\n",
    "print(display_results_retriever(\"Retriever Results\", eval_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_memory_retriever = vector_index.as_retriever(\n",
    "similarity_top_k=10, vector_store_kwargs={\"deep_memory\": True}\n",
    ")\n",
    "\n",
    "dm_retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "    [\"mrr\", \"hit_rate\"], retriever=deep_memory_retriever\n",
    ")\n",
    "dm_eval_results = await dm_retriever_evaluator.aevaluate_dataset(test_dataset)\n",
    "print(display_results_retriever(\"Retriever Results\", dm_eval_results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai360",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
