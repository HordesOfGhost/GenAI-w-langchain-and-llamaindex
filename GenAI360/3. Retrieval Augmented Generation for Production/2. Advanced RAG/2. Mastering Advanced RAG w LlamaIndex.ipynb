{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../../../.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.readers import SimpleDirectoryReader\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(\"data/\").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide to chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.huggingface import HuggingFaceInferenceAPI\n",
    "from llama_index.core import set_global_tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "llm = HuggingFaceInferenceAPI(\n",
    "    model_name='mistralai/Mistral-7B-Instruct-v0.2',\n",
    "    generate_kwargs={'temperature':0.5,\"max_length\": 64,\"max_new_tokens\":512}\n",
    ")\n",
    "\n",
    "embed = set_global_tokenizer(\n",
    "    AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\").encode\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ghost\\AppData\\Local\\Temp\\ipykernel_8464\\920291023.py:3: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(chunk_size=512, chunk_overlap=64,llm=None, embed_model='local')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import ServiceContext\n",
    "\n",
    "service_context = ServiceContext.from_defaults(chunk_size=512, chunk_overlap=64,llm=None, embed_model='local')\n",
    "node_parser = service_context.node_parser\n",
    "\n",
    "nodes = node_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ghost\\anaconda3\\envs\\genai360\\lib\\site-packages\\deeplake\\util\\check_latest_version.py:32: UserWarning: A newer version of deeplake (3.8.21) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://thapabibek1129/LlamaIndex_paulgraham_essays already exists, loading from the storage\n"
     ]
    }
   ],
   "source": [
    "from llama_index.vector_stores.deeplake import DeepLakeVectorStore\n",
    "\n",
    "my_activeloop_org_id = \"thapabibek1129\"\n",
    "my_activeloop_dataset_name = \"LlamaIndex_paulgraham_essays\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "\n",
    "# Create an index over the documnts\n",
    "vector_store = DeepLakeVectorStore(dataset_path=dataset_path, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading data to deeplake dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:00<00:00, 430.81it/s]\n",
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://thapabibek1129/LlamaIndex_paulgraham_essays', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype      shape      dtype  compression\n",
      "  -------    -------    -------    -------  ------- \n",
      " embedding  embedding  (180, 384)  float32   None   \n",
      "    id        text      (180, 1)     str     None   \n",
      " metadata     json      (180, 1)     str     None   \n",
      "   text       text      (180, 1)     str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    }
   ],
   "source": [
    "from llama_index.core.storage.storage_context import StorageContext\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "storage_context.docstore.add_documents(nodes)\n",
    "vector_index = VectorStoreIndex(nodes, storage_context=storage_context, embed_model='local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name = 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "                                       model_kwargs = {'device':'cpu'} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False})\n",
      "  (2): Normalize()\n",
      ") model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={} multi_process=False show_progress=False\n"
     ]
    }
   ],
   "source": [
    "query_engine = vector_index.as_query_engine(streaming=True, similarity_top_k=10, llm=llm, embed_model = embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'HuggingFaceEmbeddings' object has no attribute 'get_agg_embedding_from_queries'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m streaming_response \u001b[38;5;241m=\u001b[39m \u001b[43mquery_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat does Paul Graham do?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m      3\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m streaming_response\u001b[38;5;241m.\u001b[39mprint_response_stream()\n",
      "File \u001b[1;32mc:\\Users\\Ghost\\anaconda3\\envs\\genai360\\lib\\site-packages\\llama_index\\core\\base\\base_query_engine.py:40\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[1;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     39\u001b[0m     str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_or_query_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ghost\\anaconda3\\envs\\genai360\\lib\\site-packages\\llama_index\\core\\query_engine\\retriever_query_engine.py:186\u001b[0m, in \u001b[0;36mRetrieverQueryEngine._query\u001b[1;34m(self, query_bundle)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Answer a query.\"\"\"\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    184\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mQUERY, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_bundle\u001b[38;5;241m.\u001b[39mquery_str}\n\u001b[0;32m    185\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m query_event:\n\u001b[1;32m--> 186\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_synthesizer\u001b[38;5;241m.\u001b[39msynthesize(\n\u001b[0;32m    188\u001b[0m         query\u001b[38;5;241m=\u001b[39mquery_bundle,\n\u001b[0;32m    189\u001b[0m         nodes\u001b[38;5;241m=\u001b[39mnodes,\n\u001b[0;32m    190\u001b[0m     )\n\u001b[0;32m    192\u001b[0m     query_event\u001b[38;5;241m.\u001b[39mon_end(payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mRESPONSE: response})\n",
      "File \u001b[1;32mc:\\Users\\Ghost\\anaconda3\\envs\\genai360\\lib\\site-packages\\llama_index\\core\\query_engine\\retriever_query_engine.py:142\u001b[0m, in \u001b[0;36mRetrieverQueryEngine.retrieve\u001b[1;34m(self, query_bundle)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrieve\u001b[39m(\u001b[38;5;28mself\u001b[39m, query_bundle: QueryBundle) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[NodeWithScore]:\n\u001b[1;32m--> 142\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_node_postprocessors(nodes, query_bundle\u001b[38;5;241m=\u001b[39mquery_bundle)\n",
      "File \u001b[1;32mc:\\Users\\Ghost\\anaconda3\\envs\\genai360\\lib\\site-packages\\llama_index\\core\\base\\base_retriever.py:229\u001b[0m, in \u001b[0;36mBaseRetriever.retrieve\u001b[1;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mas_trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    226\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mRETRIEVE,\n\u001b[0;32m    227\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_bundle\u001b[38;5;241m.\u001b[39mquery_str},\n\u001b[0;32m    228\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m retrieve_event:\n\u001b[1;32m--> 229\u001b[0m         nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    230\u001b[0m         nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_recursive_retrieval(query_bundle, nodes)\n\u001b[0;32m    231\u001b[0m         retrieve_event\u001b[38;5;241m.\u001b[39mon_end(\n\u001b[0;32m    232\u001b[0m             payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mNODES: nodes},\n\u001b[0;32m    233\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Ghost\\anaconda3\\envs\\genai360\\lib\\site-packages\\llama_index\\core\\indices\\vector_store\\retrievers\\retriever.py:91\u001b[0m, in \u001b[0;36mVectorIndexRetriever._retrieve\u001b[1;34m(self, query_bundle)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector_store\u001b[38;5;241m.\u001b[39mis_embedding_query:\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query_bundle\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(query_bundle\u001b[38;5;241m.\u001b[39membedding_strs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     90\u001b[0m         query_bundle\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m---> 91\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_agg_embedding_from_queries\u001b[49m(\n\u001b[0;32m     92\u001b[0m                 query_bundle\u001b[38;5;241m.\u001b[39membedding_strs\n\u001b[0;32m     93\u001b[0m             )\n\u001b[0;32m     94\u001b[0m         )\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_nodes_with_embeddings(query_bundle)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'HuggingFaceEmbeddings' object has no attribute 'get_agg_embedding_from_queries'"
     ]
    }
   ],
   "source": [
    "streaming_response = query_engine.query(\n",
    "    \"What does Paul Graham do?\"\n",
    ")\n",
    "streaming_response.print_response_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sub Question Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "query_engine = vector_index.as_query_engine(similarity_top_k=10, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
    "\n",
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=query_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"pg_essay\",\n",
    "            description=\"Paul Graham essay on What I Worked On\",\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "query_engine = SubQuestionQueryEngine.from_defaults(\n",
    "    query_engine_tools=query_engine_tools,\n",
    "    service_context=service_context,\n",
    "    use_async=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for SubQuestion\n__root__\n  SubQuestion expected dict not str (type=type_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Ghost\\anaconda3\\envs\\genai360\\lib\\site-packages\\pydantic\\main.py:522\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.parse_obj\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: dictionary update sequence element #0 has length 1; 2 is required",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mquery_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHow was Paul Grahams life different before, during, and after YC?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m      3\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m( \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>>> The final response:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, response )\n",
      "File \u001b[1;32mc:\\Users\\Ghost\\anaconda3\\envs\\genai360\\lib\\site-packages\\llama_index\\core\\base\\base_query_engine.py:40\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[1;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     39\u001b[0m     str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_or_query_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ghost\\anaconda3\\envs\\genai360\\lib\\site-packages\\llama_index\\core\\query_engine\\sub_question_query_engine.py:145\u001b[0m, in \u001b[0;36mSubQuestionQueryEngine._query\u001b[1;34m(self, query_bundle)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, query_bundle: QueryBundle) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RESPONSE_TYPE:\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    143\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mQUERY, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_bundle\u001b[38;5;241m.\u001b[39mquery_str}\n\u001b[0;32m    144\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m query_event:\n\u001b[1;32m--> 145\u001b[0m         sub_questions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_question_gen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_metadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m         colors \u001b[38;5;241m=\u001b[39m get_color_mapping([\u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sub_questions))])\n\u001b[0;32m    149\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verbose:\n",
      "File \u001b[1;32mc:\\Users\\Ghost\\anaconda3\\envs\\genai360\\lib\\site-packages\\llama_index\\core\\question_gen\\llm_generators.py:81\u001b[0m, in \u001b[0;36mLLMQuestionGenerator.generate\u001b[1;34m(self, tools, query)\u001b[0m\n\u001b[0;32m     74\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[0;32m     75\u001b[0m     prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prompt,\n\u001b[0;32m     76\u001b[0m     tools_str\u001b[38;5;241m=\u001b[39mtools_str,\n\u001b[0;32m     77\u001b[0m     query_str\u001b[38;5;241m=\u001b[39mquery_str,\n\u001b[0;32m     78\u001b[0m )\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prompt\u001b[38;5;241m.\u001b[39moutput_parser \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m parse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m parse \u001b[38;5;241m=\u001b[39m cast(StructuredOutput, parse)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parse\u001b[38;5;241m.\u001b[39mparsed_output\n",
      "File \u001b[1;32mc:\\Users\\Ghost\\anaconda3\\envs\\genai360\\lib\\site-packages\\llama_index\\core\\question_gen\\output_parser.py:21\u001b[0m, in \u001b[0;36mSubQuestionOutputParser.parse\u001b[1;34m(self, output)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m json_dict:\n\u001b[0;32m     19\u001b[0m     json_dict \u001b[38;5;241m=\u001b[39m json_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 21\u001b[0m sub_questions \u001b[38;5;241m=\u001b[39m [SubQuestion\u001b[38;5;241m.\u001b[39mparse_obj(item) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m json_dict]\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StructuredOutput(raw_output\u001b[38;5;241m=\u001b[39moutput, parsed_output\u001b[38;5;241m=\u001b[39msub_questions)\n",
      "File \u001b[1;32mc:\\Users\\Ghost\\anaconda3\\envs\\genai360\\lib\\site-packages\\llama_index\\core\\question_gen\\output_parser.py:21\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m json_dict:\n\u001b[0;32m     19\u001b[0m     json_dict \u001b[38;5;241m=\u001b[39m json_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 21\u001b[0m sub_questions \u001b[38;5;241m=\u001b[39m [\u001b[43mSubQuestion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m json_dict]\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StructuredOutput(raw_output\u001b[38;5;241m=\u001b[39moutput, parsed_output\u001b[38;5;241m=\u001b[39msub_questions)\n",
      "File \u001b[1;32mc:\\Users\\Ghost\\anaconda3\\envs\\genai360\\lib\\site-packages\\pydantic\\main.py:525\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.parse_obj\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for SubQuestion\n__root__\n  SubQuestion expected dict not str (type=type_error)"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"How was Paul Grahams life different before, during, and after YC?\"\n",
    ")\n",
    "print( \">>> The final response:\\n\", response )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Retriever Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "import os\n",
    "\n",
    "os.environ['COHERE_API_KEY'] = \"<YOUR_COHERE_API_KEY>\"\n",
    "\n",
    "# Get your cohere API key on: www.cohere.com\n",
    "co = cohere.Client(os.environ['COHERE_API_KEY'])\n",
    "\n",
    "# Example query and passages\n",
    "query = \"What is the capital of the United States?\"\n",
    "documents = [\n",
    "   \"Carson City is the capital city of the American state of Nevada. At the  2010 United States Census, Carson City had a population of 55,274.\",\n",
    "   \"The Commonwealth of the Northern Mariana Islands is a group of islands in the Pacific Ocean that are a political division controlled by the United States. Its capital is Saipan.\",\n",
    "   \"Charlotte Amalie is the capital and largest city of the United States Virgin Islands. It has about 20,000 people. The city is on the island of Saint Thomas.\",\n",
    "   \"Washington, D.C. (also known as simply Washington or D.C., and officially as the District of Columbia) is the capital of the United States. It is a federal district. \",\n",
    "   \"Capital punishment (the death penalty) has existed in the United States since before the United States was a country. As of 2017, capital punishment is legal in 30 of the 50 states.\",\n",
    "   \"North Dakota is a state in the United States. 672,591 people lived in North Dakota in the year 2010. The capital and seat of government is Bismarck.\"\n",
    "   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = co.rerank(query=query, documents=docs, top_n=3, model='rerank-english-v2.0') # Change top_n to change the number of results returned. If top_n is not passed, all results will be returned.\n",
    "for idx, r in enumerate(results):\n",
    "  print(f\"Document Rank: {idx + 1}, Document Index: {r.index}\")\n",
    "  print(f\"Document: {r.document['text']}\")\n",
    "  print(f\"Relevance Score: {r.relevance_score:.2f}\")\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Document Rank: 1, Document Index: 3\n",
    "Document: Washington, D.C. (also known as simply Washington or D.C., and officially as the District of Columbia) is the capital of the United States. It is a federal district. The President of the USA and many major national government offices are in the territory. This makes it the political center of the United States of America.\n",
    "Relevance Score: 0.99\n",
    "\n",
    "Document Rank: 2, Document Index: 1\n",
    "Document: The Commonwealth of the Northern Mariana Islands is a group of islands in the Pacific Ocean that are a political division controlled by the United States. Its capital is Saipan.\n",
    "Relevance Score: 0.30\n",
    "\n",
    "Document Rank: 3, Document Index: 5\n",
    "Document: Capital punishment (the death penalty) has existed in the United States since before the United States was a country. As of 2017, capital punishment is legal in 30 of the 50 states. The federal government (including the United States military) also uses capital punishment.\n",
    "Relevance Score: 0.27\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "\n",
    "cohere_rerank = CohereRerank(api_key=os.environ['COHERE_API_KEY'], top_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = vector_index.as_query_engine(\n",
    "    similarity_top_k=10,\n",
    "    node_postprocessors=[cohere_rerank],\n",
    ")\n",
    "\n",
    "response = query_engine.query(\n",
    "    \"What did Sam Altman do in this essay?\",\n",
    ")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai360",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
