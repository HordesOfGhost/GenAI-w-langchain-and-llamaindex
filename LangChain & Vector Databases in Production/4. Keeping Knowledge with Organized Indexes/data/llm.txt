AI-generated content is becoming increasingly sophisticated, making it challenging to distinguish between genuine and computer-generated text.Fraud emails and 
Fake news are becoming everyday's story.My project aims to tackle this issue by leveraging the power of BERT (Bidirectional Encoder Representations from Transformers) to identify and 
flag AI-generated text segments. This would allow the AI to advance at much faster pace but not at the expense of human security and fundamental integrity.

The solution follows a comprehensive approach to detect AI-generated texts ðŸ’¡

Data Preprocessing: We clean and preprocess the textual data, removing the noise, stop words, puntuations and non-alphabatical words. This was done using Bert-Preprocess

Additional Datasets I collected various datasets which were hosted by various competitions like the one in analysis, and concatinated them and used the collective data to train my model. This increased my training instances form 1378 to around 50k. Allowing my model to train on a large set of varrying data allowing effective feature identification and implementation.

Model Training: Using a BERT-based sequence classification model, we train the system to distinguish between genuine and AI-generated text with a high degree of accuracy. Here I used bert-base-uncase, one may also use bert-large-cased but that may significantly increase the training time as it has 24 encoder layers instead of 12 in the case of base version, which would reduce the submission score.

Predictions: Once trained, the model generates predictions for test data, highlighting potential AI-generated content segments. The predictions have been done upon the test data.

Result Analysis: The results are then saved in a CSV file, which was then submitted in the contest.