{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load LLM Models from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "504c28061b4d435ba97e81c583a3c660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ghost\\anaconda3\\envs\\genai360\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'InferenceApi' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub, LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain_community.chat_models.huggingface import ChatHuggingFace\n",
    "\n",
    "# initialize Hub LLM\n",
    "llm_t5 = HuggingFaceHub(\n",
    "    repo_id='google/flan-t5-large',\n",
    "    model_kwargs={'temperature':0,\"max_length\": 64,\"max_new_tokens\":128}\n",
    ")\n",
    "\n",
    "llm_mistral = HuggingFaceHub(\n",
    "    repo_id='mistralai/Mistral-7B-Instruct-v0.2',\n",
    "    model_kwargs={'temperature':0.5,\"max_length\": 64,\"max_new_tokens\":512}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! repo_id is not default parameter.\n",
      "                    repo_id was transferred to model_kwargs.\n",
      "                    Please confirm that repo_id is what you intended.\n",
      "WARNING! task is not default parameter.\n",
      "                    task was transferred to model_kwargs.\n",
      "                    Please confirm that task is what you intended.\n",
      "WARNING! huggingfacehub_api_token is not default parameter.\n",
      "                    huggingfacehub_api_token was transferred to model_kwargs.\n",
      "                    Please confirm that huggingfacehub_api_token is what you intended.\n",
      "WARNING! repo_id is not default parameter.\n",
      "                    repo_id was transferred to model_kwargs.\n",
      "                    Please confirm that repo_id is what you intended.\n",
      "WARNING! task is not default parameter.\n",
      "                    task was transferred to model_kwargs.\n",
      "                    Please confirm that task is what you intended.\n",
      "WARNING! huggingfacehub_api_token is not default parameter.\n",
      "                    huggingfacehub_api_token was transferred to model_kwargs.\n",
      "                    Please confirm that huggingfacehub_api_token is what you intended.\n"
     ]
    }
   ],
   "source": [
    "chat_t5 = ChatHuggingFace(llm=llm_t5, type = \"chat\")\n",
    "chat_mistral = ChatHuggingFace(llm=llm_mistral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage,SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages = [\n",
    "#     SystemMessage(content=\"You're a helpful assistant\", additional_kwargs={}),\n",
    "#     HumanMessage(\n",
    "#         content=\"What happens when an unstoppable force meets an immovable object?\"\n",
    "#     , additional_kwargs={}, example=False),\n",
    "# ]\n",
    "# messages\n",
    "# chat_prompt = ChatPromptTemplate.from_messages(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response_t5 = chat_t5._to_chat_prompt(messages)\n",
    "# # response_mistral = chat_mistral._to_chat_prompt(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>assistant\n",
      "You're a helpful assistant<|im_end|>\n",
      "<|im_start|>user\n",
      "What happens when an unstoppable force meets an immovable object?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response_t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = chat_t5.invoke(messages)\n",
    "# print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['System: You are an assistant that helps users find information about movies.', 'Human: Find information about the movie Inception.']\n"
     ]
    }
   ],
   "source": [
    "movie_info_str = \"System: You are an assistant that helps users find information about movies.\\nHuman: Find information about the movie Inception.\"\n",
    "\n",
    "# Split the string into a list using newline character '\\n'\n",
    "movie_info_list = movie_info_str.split(\"\\n\")\n",
    "\n",
    "# Printing the list\n",
    "print(movie_info_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load the summarization chain\n",
    "summarize_chain_t5 = load_summarize_chain(llm_t5)\n",
    "summarize_chain_mistral = load_summarize_chain(llm_mistral)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the document using PyPDFLoader\n",
    "document_loader = PyPDFLoader(file_path=\"movie.pdf\")\n",
    "document = document_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The story of a guy who rides his scooter in front of the Big Mart.\n"
     ]
    }
   ],
   "source": [
    "# Summarize the document\n",
    "summary = summarize_chain_t5(document)\n",
    "print(summary['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The story revolves around a guy named Anil, who works at Big Mart. He has a secret crush on a woman named Dilasha, who often comes to the store to shop with her boyfriend. Anil is always nervous whenever they come together, and he is unable to express his feelings to Dilasha. One day, while Dilasha is shopping, she buys some chocolates, chips, and Coca-Cola, and Anil is left confused as he has seen her do the same thing the previous day with a man who is not Dilasha's boyfriend. Anil becomes suspicious and follows Dilasha and the man out of the store. He later discovers that the man is Dilasha's brother. Anil is relieved and continues to work at the store, but he cannot help but feel disappointed that he had mistaken Dilasha's brother for her boyfriend. One evening, Anil goes home and searches for Dilasha on Facebook. He finds her profile and her recent post of Rakshya Bandhan, where she is seen with her brother. Anil is happy to know the truth and sends a friend request to Dilasha's brother, Dhiraj. The next morning, Dhiraj comes to the store, and Anil greets him with a smile. They have an awkward conversation, but Anil is happy to have finally met Dilasha's brother and put his confusion to rest. The story ends with Anil continuing to work at the store, but with a newfound sense of relief and closure.\n"
     ]
    }
   ],
   "source": [
    "summary = summarize_chain_mistral(document)\n",
    "print(summary['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt = PromptTemplate(template=\"Question: {question}\\nAnswer:\", input_variables=[\"question\"])\n",
    "\n",
    "qa_t5 = LLMChain(llm=llm_t5, prompt=prompt)\n",
    "qa_mistral = LLMChain(llm=llm_mistral, prompt=prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------T5---------\n",
      "the purpose of life\n",
      "-----------Mistral-----------\n",
      " I cannot provide a definitive answer to that question as it is a philosophical and metaphysical question that has puzzled humans for centuries. Different people, cultures, and religions have their own beliefs and interpretations about the meaning of life. Some people believe that the meaning of life is to seek happiness, knowledge, or personal growth, while others believe that it is to serve a higher power or to fulfill a specific purpose. Ultimately, the meaning of life is a personal and subjective question that each individual must answer for themselves based on their own beliefs, values, and experiences.\n"
     ]
    }
   ],
   "source": [
    "print(\"------T5---------\")\n",
    "print(qa_t5.run(\"what is the meaning of life?\"))\n",
    "print(\"-----------Mistral-----------\")\n",
    "print(qa_mistral.run(\"what is the meaning of life?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai360",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
