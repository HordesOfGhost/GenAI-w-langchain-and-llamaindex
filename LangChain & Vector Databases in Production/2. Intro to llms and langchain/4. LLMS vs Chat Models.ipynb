{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../../.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ghost\\anaconda3\\envs\\genai360\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'InferenceApi' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub, LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# initialize Hub LLM\n",
    "llm_t5 = HuggingFaceHub(\n",
    "    repo_id='google/flan-t5-large',\n",
    "    model_kwargs={'temperature':0,\"max_length\": 64,\"max_new_tokens\":128}\n",
    ")\n",
    "\n",
    "llm_mistral = HuggingFaceHub(\n",
    "    repo_id='mistralai/Mistral-7B-Instruct-v0.2',\n",
    "    model_kwargs={'temperature':0.5,\"max_length\": 64,\"max_new_tokens\":512}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "  input_variables=[\"product\"],\n",
    "  template=\"What is a good name for a company that makes {product}?\",\n",
    ")\n",
    "\n",
    "chain_t5 = LLMChain(llm=llm_t5, prompt=prompt)\n",
    "chain_mistral = LLMChain(llm=llm_mistral, prompt=prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----T5------\n",
      "iHeartMedia\n",
      "-----Mistral------\n",
      "\n",
      "\n",
      "1. SonicSphere\n",
      "2. AirWaves\n",
      "3. WirelessWonders\n",
      "4. SoundSolace\n",
      "5. PulsePods\n",
      "6. FreedomFones\n",
      "7. ClearSoundz\n",
      "8. EchoEarbuds\n",
      "9. HearHere\n",
      "10. BlissBuds\n",
      "\n",
      "These are just a few suggestions, but I think they all have a nice ring to them and convey the idea of wireless headphones or earbuds. You could also consider adding a specific feature or benefit to the name, such as \"NoiseCancellationHeadphones\" or \"LongLastingBuds\" to make it more descriptive. Ultimately, the name should be memorable, easy to pronounce, and convey a sense of high-quality audio and wireless technology.\n"
     ]
    }
   ],
   "source": [
    "print(\"-----T5------\")\n",
    "print( chain_t5.run(\"wireless headphones\") )\n",
    "\n",
    "print(\"-----Mistral------\")\n",
    "print( chain_mistral.run(\"wireless headphones\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models.huggingface import ChatHuggingFace\n",
    "from langchain_core.messages import HumanMessage,SystemMessage,AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ghost\\anaconda3\\envs\\genai360\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'InferenceApi' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "llm_t5 = HuggingFaceHub(\n",
    "    repo_id='google/flan-t5-large',\n",
    "    model_kwargs={'temperature':0,\"max_length\": 64,\"max_new_tokens\":128,\"return_token_type_ids\":False}\n",
    ")\n",
    "\n",
    "llm_mistral = HuggingFaceHub(\n",
    "    repo_id='mistralai/Mistral-7B-Instruct-v0.2',\n",
    "    model_kwargs={'temperature':0.5,\"max_length\": 64,\"max_new_tokens\":512,\"return_token_type_ids\":False}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! repo_id is not default parameter.\n",
      "                    repo_id was transferred to model_kwargs.\n",
      "                    Please confirm that repo_id is what you intended.\n",
      "WARNING! task is not default parameter.\n",
      "                    task was transferred to model_kwargs.\n",
      "                    Please confirm that task is what you intended.\n",
      "WARNING! huggingfacehub_api_token is not default parameter.\n",
      "                    huggingfacehub_api_token was transferred to model_kwargs.\n",
      "                    Please confirm that huggingfacehub_api_token is what you intended.\n",
      "WARNING! repo_id is not default parameter.\n",
      "                    repo_id was transferred to model_kwargs.\n",
      "                    Please confirm that repo_id is what you intended.\n",
      "WARNING! task is not default parameter.\n",
      "                    task was transferred to model_kwargs.\n",
      "                    Please confirm that task is what you intended.\n",
      "WARNING! huggingfacehub_api_token is not default parameter.\n",
      "                    huggingfacehub_api_token was transferred to model_kwargs.\n",
      "                    Please confirm that huggingfacehub_api_token is what you intended.\n"
     ]
    }
   ],
   "source": [
    "chat_t5 = ChatHuggingFace(llm=llm_t5)\n",
    "chat_mistral = ChatHuggingFace(llm=llm_mistral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Hello There [/INST]I am a helpful assistant that translates English to Nepali.</s>[INST] Translate the following sentence: I love apples. [/INST] In Nepali, the sentence \"I love apples\" can be translated as \"मेरो अनुराग अप्पलेचाहिँ\" (mero anurag applechaaham). This sentence is written in the Devanagari script used in Nepali language. The script is read from right to left. If you need the sentence in a different script or have any other translation request, please let me know.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    HumanMessage(content=\"Hello There\",role=\"user\"),\n",
    "\tAIMessage(content=\"I am a helpful assistant that translates English to Nepali.\",role=\"assistant\"),\n",
    "\tHumanMessage(content=\"Translate the following sentence: I love apples.\",role=\"user\")\n",
    "]\n",
    "\n",
    "chat_mistral._to_chat_prompt(messages)\n",
    "res = chat_mistral.invoke(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages = [\n",
    "# \tSystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "# \tHumanMessage(content=\"Translate the following sentence: I love programming.\",role=\"user\")\n",
    "# ]\n",
    "# print(chat_t5._to_chat_prompt(messages))\n",
    "# print(chat_t5.invoke(messages))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai360",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
