{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Quantization (using CLI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimum-cli inc quantize --model facebook/opt-1.3b --output opt1.3b-quantized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flexible Quantization (using Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name \"aman-mehra/opt-1.3b-finetune-squad-ep-0.4-lr-2e-05-wd-0.01\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=\"./opt-1.3b\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name, cache_dir=\"./opt-1.3b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_evaluator = evaluate.evaluator(\"question-answering\")\n",
    "\n",
    "eval_dataset = load_dataset(\"squad\", split=\"validation\", cache_dir=\"./squad-ds\")\n",
    "eval_dataset = eval_dataset.select(range(64)) # Ues a subset of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "def eval_fn(model):\n",
    "    qa_pipeline.model = model\n",
    "    metrics = task_evaluator.compute(model_or_pipeline=qa_pipeline, data=eval_dataset, metric=\"squad\")\n",
    "    return metrics[\"f1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the accepted accuracy loss to 1%\n",
    "accuracy_criterion = AccuracyCriterion(tolerable_loss=0.01)\n",
    "\n",
    "# Set the maximum number of trials to 10\n",
    "tuning_criterion = TuningCriterion(max_trials=10)\n",
    "\n",
    "quantization_config = PostTrainingQuantConfig(\n",
    "    approach=\"dynamic\", accuracy_criterion=accuracy_criterion, tuning_criterion=tuning_criterion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer = INCQuantizer.from_pretrained(model, eval_fn=eval_fn)\n",
    "\n",
    "quantizer.quantize(quantization_config=quantization_config, save_directory=\"opt1.3b-quantized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.intel import INCModelForCausalLM\n",
    "\n",
    "model = INCModelForCausalLM.from_pretrained(\"./opt1.3b-quantized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"<PROMPT>\", return_tensors=\"pt\")\n",
    "\n",
    "generation_output = model.generate(**inputs,\n",
    "                                   return_dict_in_generate=True,\n",
    "                                   output_scores=True,\n",
    "                                   min_length=512,\n",
    "                                   max_length=512,\n",
    "                                   num_beams=1,\n",
    "                                   do_sample=True,\n",
    "                                   repetition_penalty=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( tokenizer.decode(generation_output.sequences[0]) )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
