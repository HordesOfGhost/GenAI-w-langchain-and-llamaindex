{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # poor man's data loader\n",
    "data_dir = os.path.join('data', dataset)\n",
    "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dataset\n",
    "ds = deeplake.empty(path, overwrite=True)\n",
    "\n",
    "ds.create_tensor('text', htype=\"text\", chunk_compression='lz4')\n",
    "ds.create_tensor('tokens', dtype=np.uint16, chunk_compression='lz4')\n",
    "\n",
    "@deeplake.compute\n",
    "def tokenize(example, ds):\n",
    "    ids = enc.encode_ordinary(example) # encode_ordinary ignores any special tokens\n",
    "    ids.append(enc.eot_token) # add the end of text token, e.g. 50256 for gpt2 bpe\n",
    "    ds.append({\"text\": example, \"tokens\": np.array(ids).astype(np.uint16)})\n",
    "\n",
    "# we now want to tokenize the dataset. first define the encoding function (gpt2 bpe)\n",
    "\n",
    "tokenize().eval(split_dataset[split]['text'], ds, num_workers=num_proc, scheduler=\"processed\")\n",
    "ds.commit()\n",
    "ds.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data: List[np.ndarray]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "  \"\"\" Collate function samples from a batch of documents \"\"\"\n",
    "  #concatenate all the tokens from the batch\n",
    "  data = [d['tokens'] for d in data]\n",
    "  data = np.concatenate(data, axis=0)\n",
    "\n",
    "  #sample a random block of from concatenated documents\n",
    "  ix = torch.randint(max(len(data) - block_size, 1), (batch_size,))\n",
    "  local_block_size = min(block_size, len(data)-1)\n",
    "\n",
    "  x = torch.stack(\n",
    "[torch.from_numpy((data[i:i+local_block_size]).astype(np.int64)) for i in ix])\n",
    "  y = torch.stack(\n",
    "[torch.from_numpy((data[i+1:i+1+local_block_size]).astype(np.int64)) for i in ix])\n",
    "  return x, y\n",
    "\n",
    "def get_dataloader(split: deeplake.Dataset, shuffle: bool = False, coef: float = 2, num_workers: int = 1):\n",
    "\"\"\" Returns a dataloader for the given split. Uses fast enterprise dataloader if available\"\"\"\n",
    "return dataloader(split)\\\n",
    "   .batch(int(coef*batch_size), drop_last=True)\\\n",
    "   .shuffle(shuffle)\\\n",
    "      .pytorch(num_workers=num_workers, tensors=['tokens'], collate_fn=collate_fn, distributed=ddp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = deeplake.load(dataset, read_only=True, token=token)\n",
    "ds.checkout(branch)\n",
    "\n",
    "meta_vocab_size = None\n",
    "\n",
    "n_tokens = sum(ds._tokens_shape.numpy())\n",
    "print(f'There are ~{n_tokens[0]//10**9}B tokens in the dataset')\n",
    "\n",
    "split = int(len(ds)*train_split_ratio)\n",
    "dl = {\n",
    "  \"train\": get_dataloader(ds[:split], shuffle=shuffle, num_workers=num_workers),\n",
    "  \"val\": get_dataloader(ds[split:], shuffle=False, num_workers=1)\n",
    "}\n",
    "dl_iter = {\"train\": dl[\"train\"].__iter__(), \"val\": dl[\"val\"].__iter__()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$ python3 train.py --dataset=\"hub://activeloop/openwebtext-train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$ torchrun --standalone --nproc_per_node=8 train.py --dataset=\"hub://activeloop/openwebtext-train\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
