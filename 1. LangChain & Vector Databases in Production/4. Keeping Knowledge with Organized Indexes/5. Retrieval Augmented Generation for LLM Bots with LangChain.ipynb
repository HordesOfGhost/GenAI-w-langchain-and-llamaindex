{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../../.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Loading the Data with RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import ApifyDatasetLoader\n",
    "from langchain.utilities import ApifyWrapper\n",
    "from langchain.document_loaders.base import Document\n",
    "import os\n",
    "\n",
    "os.environ[\"APIFY_API_TOKEN\"] = db.secrets.get(\"APIFY_API_TOKEN\")\n",
    "\n",
    "apify = ApifyWrapper()\n",
    "loader = apify.call_actor(\n",
    "    actor_id=\"apify/website-content-crawler\",\n",
    "    run_input={\"startUrls\": [{\"url\": \"ENTER\\YOUR\\URL\\HERE\"}]},\n",
    "    dataset_mapping_function=lambda dataset_item: Document(\n",
    "        page_content=dataset_item[\"text\"] if dataset_item[\"text\"] else \"No content available\",\n",
    "        metadata={\n",
    "            \"source\": dataset_item[\"url\"],\n",
    "            \"title\": dataset_item[\"metadata\"][\"title\"]\n",
    "        }\n",
    "    ),\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# we split the documents into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=20, length_function=len\n",
    ")\n",
    "docs_split = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.cohere import CohereEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "import os\n",
    "\n",
    "os.environ[\"COHERE_API_KEY\"] = db.secrets.get(\"COHERE_API_KEY\")\n",
    "os.environ[\"ACTIVELOOP_TOKEN\"] = db.secrets.get(\"APIFY_API_TOKEN\")\n",
    "\n",
    "embeddings = CohereEmbeddings(model = \"embed-english-v2.0\")\n",
    "\n",
    "username = \"elleneal\" # replace with your username from app.activeloop.ai\n",
    "db_id = 'kb-material'# replace with your database name\n",
    "DeepLake.force_delete_by_path(f\"hub://{username}/{db_id}\")\n",
    "\n",
    "dbs = DeepLake(dataset_path=f\"hub://{username}/{db_id}\", embedding_function=embeddings)\n",
    "dbs.add_documents(docs_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Retrieve Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.embeddings.cohere import CohereEmbeddings\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "import os\n",
    "\n",
    "os.environ[\"COHERE_API_KEY\"] = db.secrets.get(\"COHERE_API_KEY\")\n",
    "os.environ[\"ACTIVELOOP_TOKEN\"] = db.secrets.get(\"ACTIVELOOP_TOKEN\")\n",
    "\n",
    "@st.cache_resource()\n",
    "def data_lake():\n",
    "    embeddings = CohereEmbeddings(model = \"embed-english-v2.0\")\n",
    "\n",
    "    dbs = DeepLake(\n",
    "        dataset_path=\"hub://elleneal/activeloop-material\", \n",
    "        read_only=True, \n",
    "        embedding_function=embeddings\n",
    "        )\n",
    "    retriever = dbs.as_retriever()\n",
    "    retriever.search_kwargs[\"distance_metric\"] = \"cos\"\n",
    "    retriever.search_kwargs[\"fetch_k\"] = 20\n",
    "    retriever.search_kwargs[\"maximal_marginal_relevance\"] = True\n",
    "    retriever.search_kwargs[\"k\"] = 20\n",
    "\n",
    "    compressor = CohereRerank(\n",
    "        model = 'rerank-english-v2.0',\n",
    "        top_n=5\n",
    "        )\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=compressor, base_retriever=retriever\n",
    "        )\n",
    "    return dbs, compression_retriever, retriever\n",
    "\n",
    "dbs, compression_retriever, retriever = data_lake()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Use ConversationBufferWindowMemory to Build Conversation Chain with Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@st.cache_resource()\n",
    "def memory():\n",
    "    memory=ConversationBufferWindowMemory(\n",
    "        k=3,\n",
    "        memory_key=\"chat_history\",\n",
    "        return_messages=True, \n",
    "        output_key='answer'\n",
    "        )\n",
    "    return memory\n",
    "\n",
    "memory=memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "llm=llm,\n",
    "retriever=compression_retriever,\n",
    "memory=memory,\n",
    "verbose=True,\n",
    "chain_type=\"stuff\",\n",
    "return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Build The Chat UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a button to trigger the clearing of cache and session states\n",
    "if st.sidebar.button(\"Start a New Chat Interaction\"):\n",
    "    clear_cache_and_session()\n",
    "\n",
    "# Initialize chat history\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "# Display chat messages from history on app rerun\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_ui(qa):\n",
    "    # Accept user input\n",
    "    if prompt := st.chat_input(\n",
    "        \"Ask me questions: How can I retrieve data from Deep Lake in Langchain?\"\n",
    "    ):\n",
    "\n",
    "        # Add user message to chat history\n",
    "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "        # Display user message in chat message container\n",
    "        with st.chat_message(\"user\"):\n",
    "            st.markdown(prompt)\n",
    "\n",
    "        # Display assistant response in chat message container\n",
    "        with st.chat_message(\"assistant\"):\n",
    "            message_placeholder = st.empty()\n",
    "            full_response = \"\"\n",
    "\n",
    "            # Load the memory variables, which include the chat history\n",
    "            memory_variables = memory.load_memory_variables({})\n",
    "\n",
    "            # Predict the AI's response in the conversation\n",
    "            with st.spinner(\"Searching course material\"):\n",
    "                response = capture_and_display_output(\n",
    "                    qa, ({\"question\": prompt, \"chat_history\": memory_variables})\n",
    "                )\n",
    "\n",
    "            # Display chat response\n",
    "            full_response += response[\"answer\"]\n",
    "            message_placeholder.markdown(full_response + \"▌\")\n",
    "            message_placeholder.markdown(full_response)\n",
    "\n",
    "            #Display top 2 retrieved sources\n",
    "            source = response[\"source_documents\"][0].metadata\n",
    "            source2 = response[\"source_documents\"][1].metadata\n",
    "            with st.expander(\"See Resources\"):\n",
    "                st.write(f\"Title: {source['title'].split('·')[0].strip()}\")\n",
    "                st.write(f\"Source: {source['source']}\")\n",
    "                st.write(f\"Relevance to Query: {source['relevance_score'] * 100}%\")\n",
    "                st.write(f\"Title: {source2['title'].split('·')[0].strip()}\")\n",
    "                st.write(f\"Source: {source2['source']}\")\n",
    "                st.write(f\"Relevance to Query: {source2['relevance_score'] * 100}%\")\n",
    "\n",
    "        # Append message to session state\n",
    "        st.session_state.messages.append(\n",
    "            {\"role\": \"assistant\", \"content\": full_response}\n",
    "        )\n",
    "\n",
    "# Run function passing the ConversationalRetrievalChain\n",
    "chat_ui(qa)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import databutton as db\n",
    "import streamlit as st\n",
    "import io\n",
    "import re\n",
    "import sys\n",
    "from typing import Any, Callable\n",
    "\n",
    "def capture_and_display_output(func: Callable[..., Any], args, **kwargs) -> Any:\n",
    "    # Capture the standard output\n",
    "    original_stdout = sys.stdout\n",
    "    sys.stdout = output_catcher = io.StringIO()\n",
    "\n",
    "    # Run the given function and capture its output\n",
    "    response = func(args, **kwargs)\n",
    "\n",
    "    # Reset the standard output to its original value\n",
    "    sys.stdout = original_stdout\n",
    "\n",
    "    # Clean the captured output\n",
    "    output_text = output_catcher.getvalue()\n",
    "    clean_text = re.sub(r\"\\x1b[.?[@-~]\", \"\", output_text)\n",
    "\n",
    "    # Custom CSS for the response box\n",
    "    st.markdown(\"\"\"\n",
    "    <style>\n",
    "        .response-value {\n",
    "            border: 2px solid #6c757d;\n",
    "            border-radius: 5px;\n",
    "            padding: 20px;\n",
    "            background-color: #f8f9fa;\n",
    "            color: #3d3d3d;\n",
    "            font-size: 20px;  # Change this value to adjust the text size\n",
    "            font-family: monospace;\n",
    "        }\n",
    "    </style>\n",
    "    \"\"\", unsafe_allow_html=True)\n",
    "\n",
    "    # Create an expander titled \"See Verbose\"\n",
    "    with st.expander(\"See Langchain Thought Process\"):\n",
    "        # Display the cleaned text in Streamlit as code\n",
    "        st.code(clean_text)\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai360",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
