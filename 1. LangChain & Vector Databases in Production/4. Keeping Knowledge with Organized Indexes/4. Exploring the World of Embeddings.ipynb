{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../../.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Search and Vector Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar document to the query 'A dog is sitting on a yard.':\n",
      "The dog is in the yard.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Define the documents\n",
    "documents = [\n",
    "    \"The cat is on the mat.\",\n",
    "    \"There is a cat on the mat.\",\n",
    "    \"The dog is in the yard.\",\n",
    "    \"There is a dog in the yard.\",\n",
    "]\n",
    "\n",
    "# Initialize the OpenAIEmbeddings instance\n",
    "embeddings = HuggingFaceEmbeddings(model_name = 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "                                       model_kwargs = {'device':'cpu'} )\n",
    "\n",
    "# Generate embeddings for the documents\n",
    "document_embeddings = embeddings.embed_documents(documents)\n",
    "\n",
    "# Perform a similarity search for a given query\n",
    "query = \"A dog is sitting on a yard.\"\n",
    "query_embedding = embeddings.embed_query(query)\n",
    "\n",
    "# Calculate similarity scores\n",
    "similarity_scores = cosine_similarity([query_embedding], document_embeddings)[0]\n",
    "\n",
    "# Find the most similar document\n",
    "most_similar_index = np.argmax(similarity_scores)\n",
    "most_similar_document = documents[most_similar_index]\n",
    "\n",
    "print(f\"Most similar document to the query '{query}':\")\n",
    "print(most_similar_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "hf = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
    "\n",
    "documents = [\"Document 1\", \"Document 2\", \"Document 3\"]\n",
    "doc_embeddings = hf.embed_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohere embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cohere\n",
      "  Downloading cohere-4.46-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: aiohttp<4.0,>=3.0 in c:\\users\\ghost\\anaconda3\\envs\\genai360\\lib\\site-packages (from cohere) (3.9.3)\n",
      "Requirement already satisfied: backoff<3.0,>=2.0 in c:\\users\\ghost\\anaconda3\\envs\\genai360\\lib\\site-packages (from cohere) (2.2.1)\n",
      "Collecting fastavro<2.0,>=1.8 (from cohere)\n",
      "  Downloading fastavro-1.9.3-cp39-cp39-win_amd64.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: importlib_metadata<7.0,>=6.0 in c:\\users\\ghost\\anaconda3\\envs\\genai360\\lib\\site-packages (from cohere) (6.11.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.25.0 in c:\\users\\ghost\\anaconda3\\envs\\genai360\\lib\\site-packages (from cohere) (2.31.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\ghost\\anaconda3\\envs\\genai360\\lib\\site-packages (from cohere) (1.26.18)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ghost\\anaconda3\\envs\\genai360\\lib\\site-packages (from aiohttp<4.0,>=3.0->cohere) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ghost\\anaconda3\\envs\\genai360\\lib\\site-packages (from aiohttp<4.0,>=3.0->cohere) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ghost\\anaconda3\\envs\\genai360\\lib\\site-packages (from aiohttp<4.0,>=3.0->cohere) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ghost\\anaconda3\\envs\\genai360\\lib\\site-packages (from aiohttp<4.0,>=3.0->cohere) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\ghost\\anaconda3\\envs\\genai360\\lib\\site-packages (from aiohttp<4.0,>=3.0->cohere) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\ghost\\anaconda3\\envs\\genai360\\lib\\site-packages (from aiohttp<4.0,>=3.0->cohere) (4.0.3)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\ghost\\anaconda3\\envs\\genai360\\lib\\site-packages (from importlib_metadata<7.0,>=6.0->cohere) (3.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ghost\\anaconda3\\envs\\genai360\\lib\\site-packages (from requests<3.0.0,>=2.25.0->cohere) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ghost\\anaconda3\\envs\\genai360\\lib\\site-packages (from requests<3.0.0,>=2.25.0->cohere) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ghost\\anaconda3\\envs\\genai360\\lib\\site-packages (from requests<3.0.0,>=2.25.0->cohere) (2024.2.2)\n",
      "Downloading cohere-4.46-py3-none-any.whl (52 kB)\n",
      "   ---------------------------------------- 0.0/52.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/52.2 kB ? eta -:--:--\n",
      "   ------- -------------------------------- 10.2/52.2 kB ? eta -:--:--\n",
      "   ----------------------- ---------------- 30.7/52.2 kB 325.1 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 52.2/52.2 kB 385.0 kB/s eta 0:00:00\n",
      "Downloading fastavro-1.9.3-cp39-cp39-win_amd64.whl (546 kB)\n",
      "   ---------------------------------------- 0.0/546.2 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 30.7/546.2 kB 1.4 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 61.4/546.2 kB 1.1 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 174.1/546.2 kB 1.3 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 204.8/546.2 kB 1.1 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 317.4/546.2 kB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 358.4/546.2 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  542.7/546.2 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 546.2/546.2 kB 1.6 MB/s eta 0:00:00\n",
      "Installing collected packages: fastavro, cohere\n",
      "Successfully installed cohere-4.46 fastavro-1.9.3\n"
     ]
    }
   ],
   "source": [
    "!pip install cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Hello from Cohere!\n",
      "Embedding: [0.23461914, 0.50146484, -0.048828125, 0.13989258, -0.18029785]\n",
      "Text: مرحبًا من كوهير!\n",
      "Embedding: [0.25317383, 0.30004883, 0.0104904175, 0.12573242, -0.18273926]\n",
      "Text: Hallo von Cohere!\n",
      "Embedding: [0.10266113, 0.28320312, -0.050201416, 0.23706055, -0.07159424]\n",
      "Text: Bonjour de Cohere!\n",
      "Embedding: [0.15185547, 0.28173828, -0.057281494, 0.11743164, -0.04385376]\n",
      "Text: ¡Hola desde Cohere!\n",
      "Embedding: [0.25146484, 0.43139648, -0.0859375, 0.24682617, -0.11706543]\n",
      "Text: Olá do Cohere!\n",
      "Embedding: [0.18664551, 0.39038086, -0.045898438, 0.14562988, -0.11254883]\n",
      "Text: Ciao da Cohere!\n",
      "Embedding: [0.115722656, 0.43310547, -0.026168823, 0.14575195, 0.07080078]\n",
      "Text: 您好，来自 Cohere！\n",
      "Embedding: [0.24609375, 0.30859375, -0.111694336, 0.26635742, -0.051086426]\n",
      "Text: कोहेरे से नमस्ते!\n",
      "Embedding: [0.1932373, 0.6352539, 0.03213501, 0.117370605, -0.26098633]\n"
     ]
    }
   ],
   "source": [
    "import cohere\n",
    "from langchain.embeddings import CohereEmbeddings\n",
    "\n",
    "# Initialize the CohereEmbeddings object\n",
    "cohere = CohereEmbeddings(\n",
    "\tmodel=\"embed-multilingual-v2.0\",\n",
    "\tcohere_api_key=\"key\"\n",
    ")\n",
    "\n",
    "# Define a list of texts\n",
    "texts = [\n",
    "    \"Hello from Cohere!\", \n",
    "    \"مرحبًا من كوهير!\", \n",
    "    \"Hallo von Cohere!\",  \n",
    "    \"Bonjour de Cohere!\", \n",
    "    \"¡Hola desde Cohere!\", \n",
    "    \"Olá do Cohere!\",  \n",
    "    \"Ciao da Cohere!\", \n",
    "    \"您好，来自 Cohere！\", \n",
    "    \"कोहेरे से नमस्ते!\"\n",
    "]\n",
    "\n",
    "# Generate embeddings for the texts\n",
    "document_embeddings = cohere.embed_documents(texts)\n",
    "\n",
    "# Print the embeddings\n",
    "for text, embedding in zip(texts, document_embeddings):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Embedding: {embedding[:5]}\")  # print first 5 dimensions of each embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Lake Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our documents\n",
    "texts = [\n",
    "    \"Napoleon Bonaparte was born in 15 August 1769\",\n",
    "    \"Louis XIV was born in 5 September 1638\",\n",
    "    \"Lady Gaga was born in 28 March 1986\",\n",
    "    \"Michael Jeffrey Jordan was born in 17 February 1963\"\n",
    "]\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.create_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dca2996e124e4ff68e2538798b13319d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ghost\\anaconda3\\envs\\genai360\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Ghost\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e154a1955e3043fdb19779071be4bb25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c43e2f6291ec47cfa68512af45fc6933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4cd84508a0e4c63be3836f755ea09f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2098a89ac0c7465fa2cafb05993a7756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95ac43b3a1f243ef956503ab2d0400b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02554cf4a30c4ffdaa17fb7e9c71b1e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f0b5917f9684a688f079a693812b1da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78ccb484b9cf4e69ba320bf9cbcc4012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e65a6dd63154d918dd07e5a4d4cbff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c511f94ee5904490867ac531af3cc1a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 4 embeddings in 1 batches of size 4:: 100%|██████████| 1/1 [00:34<00:00, 34.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://thapabibek1129/langchain_course_embeddings', tensors=['text', 'metadata', 'embedding', 'id'])\n",
      "\n",
      "  tensor      htype     shape     dtype  compression\n",
      "  -------    -------   -------   -------  ------- \n",
      "   text       text      (4, 1)     str     None   \n",
      " metadata     json      (4, 1)     str     None   \n",
      " embedding  embedding  (4, 384)  float32   None   \n",
      "    id        text      (4, 1)     str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ed95c016-c973-11ee-9ad5-a434d9523559',\n",
       " 'ed95c017-c973-11ee-9808-a434d9523559',\n",
       " 'ed95c018-c973-11ee-a2b9-a434d9523559',\n",
       " 'ed95c019-c973-11ee-9016-a434d9523559']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize embeddings model\n",
    "embeddings = HuggingFaceEmbeddings(model_name = 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "                                       model_kwargs = {'device':'cpu'} )\n",
    "\n",
    "\n",
    "# create Deep Lake dataset\n",
    "# TODO: use your organization id here. (by default, org id is your username)\n",
    "my_activeloop_org_id = \"thapabibek1129\"\n",
    "my_activeloop_dataset_name = \"langchain_course_embeddings\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "\n",
    "# add documents to our Deep Lake dataset\n",
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create retriever from db\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ghost\\anaconda3\\envs\\genai360\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'InferenceApi' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# initialize Hub LLM\n",
    "llm_t5 = HuggingFaceHub(\n",
    "    repo_id='google/flan-t5-large',\n",
    "    model_kwargs={'temperature':0,\"max_length\": 64,\"max_new_tokens\":128}\n",
    ")\n",
    "\n",
    "llm_mistral = HuggingFaceHub(\n",
    "    repo_id='mistralai/Mistral-7B-Instruct-v0.2',\n",
    "    model_kwargs={'temperature':0.5,\"max_length\": 64,\"max_new_tokens\":512}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 February 1963\n"
     ]
    }
   ],
   "source": [
    "# create the question-answering chain\n",
    "qa_t5 = RetrievalQA.from_llm(llm_t5, retriever=retriever)\n",
    "\n",
    "# ask a question to the chain\n",
    "print(qa_t5.run(\"When was Michael Jordan born?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Michael Jordan was born on February 17, 1963.\n",
      "\n",
      "Explanation: The question asks for the birthdate of Michael Jordan, and the context provides the correct birthdate (February 17, 1963). The helpful answer simply repeats the information from the context to ensure clarity and accuracy.\n"
     ]
    }
   ],
   "source": [
    "# create the question-answering chain\n",
    "qa_mistral = RetrievalQA.from_llm(llm_mistral, retriever=retriever)\n",
    "\n",
    "# ask a question to the chain\n",
    "print(qa_mistral.run(\"When was Michael Jordan born?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai360",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
