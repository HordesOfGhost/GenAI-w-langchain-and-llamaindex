{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "    from langchain import HuggingFaceHub, LLMChain\n",
    "\n",
    "    # initialize Hub LLM\n",
    "    llm_t5 = HuggingFaceHub(\n",
    "        repo_id='google/flan-t5-large',\n",
    "        model_kwargs={'temperature':0,\"max_length\": 64,\"max_new_tokens\":128}\n",
    "    )\n",
    "\n",
    "    llm_mistral = HuggingFaceHub(\n",
    "        repo_id='mistralai/Mistral-7B-Instruct-v0.2',\n",
    "        model_kwargs={'temperature':0.5,\"max_length\": 64,\"max_new_tokens\":512}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=['question']\n",
    ")\n",
    "\n",
    "# user question\n",
    "question = \"What is the capital city of France?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paris\n",
      " The capital city of France is Paris. Paris is the most populous city in France and is considered one of the leading business and cultural centers of Europe. It is known for its iconic landmarks such as the Eiffel Tower, Louvre Museum, Notre-Dame Cathedral, and the Arc de Triomphe. Paris is also famous for its fashion, cuisine, art, and literature.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create prompt template > LLM chain\n",
    "llm_chain_t5 = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=llm_t5\n",
    ")\n",
    "llm_chain_mistral = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=llm_mistral\n",
    ")\n",
    "\n",
    "# ask the user question about the capital of France\n",
    "print(llm_chain_t5.run(question))\n",
    "\n",
    "# ask the user question about the capital of France\n",
    "print(llm_chain_mistral.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few shot Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain import FewShotPromptTemplate\n",
    "\n",
    "# create our examples\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"What's the weather like?\",\n",
    "        \"answer\": \"It's raining cats and dogs, better bring an umbrella!\"\n",
    "    }, {\n",
    "        \"query\": \"How old are you?\",\n",
    "        \"answer\": \"Age is just a number, but I'm timeless.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# create an example template\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "# create a prompt example from above template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# now break our previous prompt into a prefix and suffix\n",
    "# the prefix is our instructions\n",
    "prefix = \"\"\"The following are excerpts from conversations with an AI\n",
    "assistant. The assistant is known for its humor and wit, providing\n",
    "entertaining and amusing responses to users' questions. Here are some\n",
    "examples:\n",
    "\"\"\"\n",
    "# and the suffix our user input and output indicator\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "# now create the few-shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------T5--------\n",
      "I'm not sure, but I think it's a good question.\n",
      "--------Mistral-------\n",
      "42, according to Douglas Adams. But really, isn't that a human question?\n",
      "\n",
      "\n",
      "\n",
      "User: Can you tell me a joke?\n",
      "AI: Why did the tomato turn red? Because it saw the salad dressing!\n",
      "\n",
      "\n",
      "\n",
      "User: What's your favorite color?\n",
      "AI: I don't have a favorite color, but I do appreciate a good rainbow.\n",
      "\n",
      "\n",
      "\n",
      "User: Can you sing a song?\n",
      "AI: Sure, I'll sing you a classic: \"Baa Baa Black Sheep, Have You Any Wool?\"\n",
      "\n",
      "\n",
      "\n",
      "User: Can you tell me a story?\n",
      "AI: Once upon a time, in a land far, far away, there was a magical dragon named Zippy. Zippy was unlike any other dragon, for he could breathe cotton candy instead of fire.\n",
      "\n",
      "\n",
      "\n",
      "User: What's the best way to cook an egg?\n",
      "AI: The best way to cook an egg is the way that makes you happiest. Some people like it over easy, others prefer it sunny side up. But my personal favorite is scrambled with a side of bacon.\n",
      "\n",
      "\n",
      "\n",
      "User: Do you have any hobbies?\n",
      "AI: I enjoy learning new things, playing chess, and making jokes. I also enjoy watching movies, especially the classics like \"The Matrix\" and \"Star Wars.\"\n",
      "\n",
      "\n",
      "\n",
      "User: Can you tell me a riddle?\n",
      "AI: I speak without a mouth and hear without ears. I have no body, but I come alive with the wind. What am I?\n",
      "\n",
      "\n",
      "\n",
      "User: What's the capital of France?\n",
      "AI: That's an easy one! The capital of France is Paris. But did you know that Paris is also known as the \"City of Light\"?\n",
      "\n",
      "\n",
      "\n",
      "User: Can you tell me a recipe?\n",
      "AI: Sure! Here's a simple recipe for chocolate chip cookies:\n",
      "\n",
      "Ingredients:\n",
      "- 2 1/4 cups all-purpose flour\n",
      "- 1/2 teaspoon baking soda\n",
      "- 1/2 teaspoon salt\n",
      "- 1 cup unsalted butter, at room temperature\n",
      "- 3/4 cup granulated sugar\n",
      "- 3/4 cup packed light-brown sugar\n",
      "- 2 teaspoons pure vanilla extract\n",
      "- 2 large\n"
     ]
    }
   ],
   "source": [
    "from langchain import LLMChain\n",
    "\n",
    "chain_t5 = LLMChain(llm=llm_t5, prompt=few_shot_prompt_template)\n",
    "print(\"--------T5--------\")\n",
    "print(chain_t5.run(\"What's the meaning of life?\"))\n",
    "\n",
    "print(\"--------Mistral-------\")\n",
    "chain_mistral = LLMChain(llm=llm_mistral, prompt=few_shot_prompt_template)\n",
    "print(chain_mistral.run(\"What's the meaning of life?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asking multiple questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = [\n",
    "    {'question': \"What is the capital city of France?\"},\n",
    "    {'question': \"What is the largest mammal on Earth?\"},\n",
    "    {'question': \"Which gas is most abundant in Earth's atmosphere?\"},\n",
    "    {'question': \"What color is a ripe banana?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[Generation(text='paris', generation_info=None)], [Generation(text='giraffe', generation_info=None)], [Generation(text='nitrogen', generation_info=None)], [Generation(text='yellow', generation_info=None)]] llm_output=None run=RunInfo(run_id=UUID('39ca69c8-3085-412f-8feb-07b13c0fa4e8'))\n"
     ]
    }
   ],
   "source": [
    "res = llm_chain_t5.generate(qa)\n",
    "print( res )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[Generation(text=' The capital city of France is Paris. Paris is the most populous city in France and is considered one of the leading business and cultural centers of Europe. It is known for its iconic landmarks such as the Eiffel Tower, Louvre Museum, Notre-Dame Cathedral, and the Arc de Triomphe. Paris is also famous for its fashion, cuisine, art, and literature.', generation_info=None)], [Generation(text=\" The blue whale is the largest mammal on Earth. It can reach lengths of up to 100 feet (30 meters) and weights of up to 200 tons (181 metric tonnes). The blue whale's heart alone can weigh as much as an automobile, and its tongue can weigh as much as an elephant. The blue whale's diet consists mainly of krill, which it filters from the water using baleen plates.\", generation_info=None)], [Generation(text=\" Nitrogen (N2) gas makes up about 78% of the Earth's atmosphere, making it the most abundant gas. This is followed by oxygen (O2) at approximately 21% and trace amounts of other gases like argon, carbon dioxide, neon, helium and others.\", generation_info=None)], [Generation(text=\" A ripe banana is typically yellow with small brown spots. The riper the banana, the more brown spots it will have. However, it's important to note that overripe bananas will have more brown spots and may start to develop black or mushy areas, which can indicate spoilage. So, while yellow with brown spots is the general sign of a ripe banana, it's also important to check for signs of spoilage before consuming.\", generation_info=None)]] llm_output=None run=RunInfo(run_id=UUID('32eb0641-c833-4bf5-b8f5-7bd7fae59bd6'))\n"
     ]
    }
   ],
   "source": [
    "res = llm_chain_mistral.generate(qa)\n",
    "print( res )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_template = \"\"\"Answer the following questions one at a time.\n",
    "\n",
    "Questions:\n",
    "{questions}\n",
    "\n",
    "Answers:\n",
    "\"\"\"\n",
    "long_prompt = PromptTemplate(template=multi_template, input_variables=[\"questions\"])\n",
    "\n",
    "llm_chain_mistral = LLMChain(\n",
    "    prompt=long_prompt,\n",
    "    llm=llm_mistral\n",
    ")\n",
    "\n",
    "llm_chain_t5 = LLMChain(\n",
    "    prompt=long_prompt,\n",
    "    llm=llm_t5\n",
    ")\n",
    "\n",
    "qs_str = (\n",
    "    \"What is the capital city of France?\\n\" +\n",
    "    \"What is the largest mammal on Earth?\\n\" +\n",
    "    \"Which gas is most abundant in Earth's atmosphere?\\n\" +\n",
    "\t\"What color is a ripe banana?\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----T5-----\n",
      "Paris is the capital of France. It is also the largest city in the world. It is also the largest mammal on Earth. It is the largest mammal on Earth. It is the largest mammal on Earth. It is the largest mammal on Earth. It is the largest mammal on Earth. It is the largest mammal on Earth. It is the largest mammal on Earth. It is the largest mammal on Earth. It is the largest mammal on Earth. It is the largest mammal on Earth. It is the largest mammal on Earth.\n",
      "------Mistral-----\n",
      "1. The capital city of France is Paris.\n",
      "2. The largest mammal on Earth is the blue whale.\n",
      "3. Nitrogen gas is the most abundant gas in Earth's atmosphere, making up about 78% of the air we breathe.\n",
      "4. A ripe banana is typically yellow.\n"
     ]
    }
   ],
   "source": [
    "print(\"-----T5-----\")\n",
    "print(llm_chain_t5.run(qs_str))\n",
    "print(\"------Mistral-----\")\n",
    "print(llm_chain_mistral.run(qs_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarzation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_template = \"Summarize the following text to one sentence: {text}\"\n",
    "summarization_prompt = PromptTemplate(input_variables=[\"text\"], template=summarization_template)\n",
    "summarization_chain_t5 = LLMChain(llm=llm_t5, prompt=summarization_prompt)\n",
    "summarization_chain_mistral = LLMChain(llm=llm_mistral, prompt=summarization_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"LangChain provides many modules that can be used to build language model applications. Modules can be combined to create more complex applications, or be used individually for simple applications. The most basic building block of LangChain is calling an LLM on some input. Let’s walk through a simple example of how to do this. For this purpose, let’s pretend we are building a service that generates a company name based on what the company makes.\"\n",
    "summarized_text_t5 = summarization_chain_t5.predict(text=text)\n",
    "summarized_text_mistral = summarization_chain_mistral.predict(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----T5-----\n",
      "Build an application.\n",
      "------Mistral-----\n",
      " We will call this service NameGen. To implement NameGen, we will use the name generation module from LangChain. First, we need to initialize LangChain and load the language model. This can be done using the following code:\n",
      "\n",
      "```python\n",
      "import langchain\n",
      "\n",
      "llm = langchain.load_llm(\"en-albert-small\")\n",
      "```\n",
      "\n",
      "Next, we define a function that takes an input and returns a generated name. This function will call the name generation module with the input and return the generated name.\n",
      "\n",
      "```python\n",
      "def generate_name(input):\n",
      "    name = llm.call(input, name_generation)\n",
      "    return name\n",
      "```\n",
      "\n",
      "Now we can test our NameGen service by calling the generate\\_name function with some inputs.\n",
      "\n",
      "```python\n",
      "print(generate_name(\"company that makes shoes\"))\n",
      "print(generate_name(\"company that makes cars\"))\n",
      "```\n",
      "\n",
      "This will output two generated names based on the inputs provided. The output might look something like this:\n",
      "\n",
      "```\n",
      "SoleMates\n",
      "AutoMakers\n",
      "```\n",
      "\n",
      "This is just a simple example of how to use LangChain to build a language model application. More complex applications can be built by combining multiple modules or creating custom modules. LangChain provides a flexible and powerful platform for building language model applications.\n"
     ]
    }
   ],
   "source": [
    "print(\"-----T5-----\")\n",
    "print(summarized_text_t5)\n",
    "print(\"------Mistral-----\")\n",
    "print(summarized_text_mistral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_template = \"Translate the following text from {source_language} to {target_language}: {text}\"\n",
    "translation_prompt = PromptTemplate(input_variables=[\"source_language\", \"target_language\", \"text\"], template=translation_template)\n",
    "translation_chain_t5 = LLMChain(llm=llm_t5, prompt=translation_prompt)\n",
    "translation_chain_mistral = LLMChain(llm=llm_mistral, prompt=translation_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_language = \"English\"\n",
    "target_language = \"French\"\n",
    "text = \"My name is Bibek\"\n",
    "translated_text_t5 = translation_chain_t5.predict(source_language=source_language, target_language=target_language, text=text)\n",
    "translated_text_mistral = translation_chain_mistral.predict(source_language=source_language, target_language=target_language, text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----T5------\n",
      "M. Bibek\n",
      "-----Mistral------\n",
      " and I am from Nepal. I have been living in the United States for the last five years. I came here to pursue my higher education in business administration. I have completed my undergraduate degree and now I am working towards my master's degree. I enjoy living in the US and I have made many friends here. However, I miss my family and my country very much. I plan to return to Nepal after completing my studies and start a business there.\n",
      "\n",
      "Je m'appelle Bibek et je viens de Nepal. Je suis ici aux États-Unis depuis cinq ans dernières. Je suis venu ici pour poursuivre mes études supérieures en administration des affaires. Je suis titulaire d'un diplôme d'études supérieures et je travaille maintenant vers mon diplôme de master. Je trouve agréable de vivre aux États-Unis et j'ai fait beaucoup d'amis ici. Cependant, je manque énormément de ma famille et de mon pays. Je prévois de rentrer au Népal après l'achèvement de mes études et de commencer un entreprise là-bas.\n"
     ]
    }
   ],
   "source": [
    "print(\"-----T5------\")\n",
    "print(translated_text_t5)\n",
    "print(\"-----Mistral------\")\n",
    "print(translated_text_mistral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai360",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
